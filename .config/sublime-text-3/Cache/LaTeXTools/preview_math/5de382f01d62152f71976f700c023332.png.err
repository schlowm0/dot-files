Failed to run 'pdflatex' to create pdf to preview.


Errors:
TeX STOPPED: File ended while scanning use of \textbf{e = (x,\ y,\ ts,\ p), $ where $x$ and $y$ define the pixel location 
TeX reports the error was in file: 5de382f01d62152f71976f700c023332.tex

LaTeX document:
-----BEGIN DOCUMENT-----

\documentclass[preview,border=0.3pt]{standalone}
% import xcolor if available and not already present
\IfFileExists{xcolor.sty}{\usepackage{xcolor}}{}%
\usepackage{amsmath}
\usepackage{amssymb}
\IfFileExists{latexsym.sty}{\usepackage{latexsym}}{}
\IfFileExists{mathtools.sty}{\usepackage{mathtools}}{}

\begin{document}
% set the foreground color
\IfFileExists{xcolor.sty}{\color[HTML]{C6C6C6}}{}%
$
\textbf{e = (x,\ y,\ ts,\ p), $ where $x$ and $y$ define the pixel location in retinal reference frame, $ts$ is the time stamp, and $p$ is the polarity of the event, i.e. whether luminance of the pixel has increased (an ``on'' event) or decreased (an ``off'' event). All pixels write to a common transmission bus, which uses the Address Event Representation (AER) protocol to communicate with either a standard computer or a neuromorphic chip. The asynchronous sampling based on events makes this sensor low power, low latency, and low on the amount of data to be transmitted. In fact, if there is no change in the visual scene, no information is transmitted off the camera. If a change is detected, it is communicated with the rate on the order of microseconds. 

For our obstacle avoidance scenario, important properties of the DVS are its high-speed, high dynamic range, and small sensitivity to lighting conditions. The challenges are noise, inherent in the sensor, its inability to detect homogeneous surfaces, and relatively small spatial resolution (128x128 pixels), as well as a limited field of view (60$\deg$). New versions of DVS are currently available, which would further improve performance of the system and allow for more soffisticated target-detection algorithms \citep{CorradiEtAl2016}. 


%When the DVS is moved in a static environment, objects induce spatio-temporal changes in contrast, e.g. events or spikes, which are sensed by the pixels. Objects are mainly perceived through their boundaries, since these are the location with the highest likelihood of spatial-temporal contrast change (Fig. \ref{fig:system_overview}, Red box). 


% Each pixel of the sensor detects a local luminance change and sends a signal about this \emph{event} off the chip using an event-address representation (AER) \cite(aer), in which the address of the event emitting pixel is transmitted, together with the polarity of the detected change (increasing or decreasing luminance) and, optionally, a time stamp. Such events are reported by the pixels asynchronously and immediately, 
% which makes this sensor low power, low latency, and low on the amount of data to be transmitted. In fact, if there's no change in visual scene, no information is transmitted off the camera. If a change is detected, it is communicated with the rate on the order of microseconds, with the USB interfacing being the limiting factor is many events happen simultaneously (upto 100K events can be transmitted without a noticeable delay). 
In this work, we used an embedded version of the DVS camera \citep{MullerEtAl2011}, in which the event-based silicon retina is integrated with am ARM7 micro-controller that initializes the DVS, captures events, sends them to the wireless network, as well as receives and processes commands for motor control.


% The asynchronous sampling offers not only a very sparse representation of the environment, but also ultra-low latency since no global sampling frequency is needed. Furthermore, the ternary (on, off, 0) nature of the events inherently represents time.

% Fig.~\ref{fig:DVSImage} shows a typical image, sample from the DVS output in a 100ms time-window during robot's motion.
% - Change figure to conference slide and exchange images (frames vs events)
% \begin{figure}
%   \centering
%   \includegraphics[width=0.45\textwidth]{Figures/DVSView.png}
%   \caption{An example of the output of a DVS: a frame produced by accumulating events for 1s. Black pixels are ``off'' events, where luminance has decreased, white pixels are ``on'' events, where luminance has increased. One can see contours of objects, where luminance change is largest when the camera moves, and some camera noise.}
%   \label{fig:DVSImage}
% \end{figure}

%low latency
%sparse, ternary representation of the environment

%objects only/mainly boundaries --> shape

%- working principle of DVS
\begin{figure}
  \centering
  \includegraphics[width=0.45\textwidth]{Figures/Robot_Controller_Scheme_MoritzEdit.pdf}
  \caption{Information flow between the three main components: ROLLS, Parallella, and Pushbot. The Parallella receives events from the DVS and sends these events directly (the only preprocessing being dropping 80\% of these events, see text for details) to the target neuron populations on the ROLLS, effectively ``wiring'' portions of the DVS sensor surface to different groups of neurons on the ROLLS. The Parallella also receives spikes from the ROLLS chip and computes the motor commands for the Pushbot based on simple counting of the spikes.}
  \label{fig:scheme}
\end{figure}

\subsection{Neuromorphic Robot}

The robot used in this work is the Pushbot (Fig. \ref{fig:system_overview}), which consists of an embedded DVS camera (eDVS), mounted on a $10 \times 10\ cm$ chassis with two motors (left and right). The robot is equipped with two parallel laser pointers and a LED on top. The communication protocol of the robot allows to set velocity for its two wheels and to receive eDVS events through a serial TCP/IP interface. The Pushbot is powered by 4 AA-batteries, which ensure $\sim$2 h operation time. The velocity for each motor (left and right wheel) can be set independently and in an event-driven manner.
% - parallela which talks to the neuromorphic processor rolls


\subsection{Spiking Neural Network Architecture}

\label{sec:NN_Architecture}

Our neuronal obstacle avoidance architecture is realised with neuronal populations on the ROLLS device. Connectivity among these populations is schematically shown in Fig.~\ref{fig:NN_Arch_obst}. 


\begin{figure}[tbh]
  \begin{minipage}[b]{.5\linewidth}	  
	\centering\includegraphics[width=\textwidth]{Figures/connectivity_obstacles.pdf}
  	\subcaption{Obstacle avoidance}
  	\label{fig:NN_Arch_obst}
  \end{minipage}
  \begin{minipage}[b]{.5\linewidth}
  	 \centering\includegraphics[width=\textwidth]{Figures/connectivity_target.pdf}
  	 \subcaption{Target acquisition}
	 \label{fig:NN_Arch_tar}	
  \end{minipage}
  \caption{The implemented neuronal architectures for obstacle avoidance and target acquisition. Violet OL and OR node are obstacle detecting neuronal populations. Orange DL and DR are motor driving populations. Violet line of nodes shows a DNF population which represent targets. Thin arrows show excitatory non-plastic connections realised on the ROLLS chip, whereas colors and numbers show the weights (the exact value of the weights is set by the biases, listed in the Appnedix). On the chip, both architectures are realised at the same time. }
  \label{fig:NN_Arch}
\end{figure}


We configured two neuronal populations (of 16 neurons each) to represent a sensed obstacle to the right and to the left from the robot's heading direction, respectively ($n_{OL}, n_{OR}$). Each neuron in the $n_{OL}$ ($n_{OR}$) population receives a spike for each DVS event that happened in the left (right) part of the sensor (we used the portion of the sensor, shown in Fig.\ref{fig:NN_Arch_obst}, left, ``DVS frame''). The neurons sum up these spikes according to their neuronal integrate-and-fire dynamics (see equations in Section~\ref{sec:appendix1}) and if enough events arrive, they start emit spikes themselves, signalling detection of an object in the respective direction (left or right). 

Each of these obstacle detecting neuronal populations is connected to a respective (reversed) ``motor'' population: drive left $n_{DL}$ and drive right $n_{DR}$ populations, so that if an obstacle is detected on the right, the robot turn lefts, and vice versa. The drive populations inhibit each other and thus implement a winner-take-all dynamics. A decision about the driving direction is made at this point. The \emph{drive} populations also inhibit the respective obstacle detecting populations, since during a turning movement of the robot, much more events are generated by the DVS than during translational movement, and this inhibition compensates for this expected increase in the input rate (similar to the motor-reafferent noise removal in biological neural systems \citep{DeanEtAl2009}). 

Controlling the speed of the robot is an important component to obstacle avoidance to enable safe navigation. Thus, we define a neuronal population $n_{sp}$, which activity controls the speed of the robot. This population receives input from a constantly firing $n_{exc}$ excitatory population, which features recurrent connections and continually fires when triggered by a transient activity pulse. This population determines a constant speed that the robot adopts when driving in an obstacles-free environment. The obstacle detecting populations $n_{OL}$ and $n_{OR}$ inhibit the speed encoding population $n_{sp}$, making the robot to slow down if obstacles are present, ensuring a safe avoidance manoeuvre. These six populations is all that is needed to implement the obstacle avoidance dynamics in our architecture.  

The control signal, sent to the robot is the angular velocity signal, calculated according to Eq.~\ref{eq:count} and the speed signal, calculated according to Eq.~\ref{eq:count_sp}: 


\begin{align} 
	&v_{a} = c_{vel} (\frac{N^{spike}_{DL}}{N^{n}_{DL}} - \frac{N^{spike}_{DR}}{N^{n}_{DLR}}), \label{eq:count} \\
	&s = c_{speed} \frac{N^{spike}_{sp}}{N^{n}_{sp}}, \label{eq:count_sp}
\end{align}
where $N^{spike}$ are number of spikes, obtained from the respective population (drive left (DL), drive right (DR), and speed (sp)) in a fixed time-window (parameter of the algorithm, we used 500 ms); $N^{n}$ is the number of neurons in the respective population; and $c_{vel}$ and $c_{speed}$ are user-specified constants. Thus, both angular and translational velocities are simply proportional to the average spike rate of the respective neuronal population. 

%of neurons are defined on the ROLLS chip to represent the directions towards the obstacles (``left'', ``right'', and ``center''). Each population has 20 spiking silicon neurons, each of which receives a stimulating spike for every camera event from the respective portion of the camera frame (see Fig.~\ref{fig:obstacles})\footnote{We drop 80\% of camera events randomly, which was proven to improve performance of obstacle avoidance, for mathematical consideration of this phenomenon, see \cite{}}. All neurons receive the same input and, in principle, behave in the same way. The neuronal populations, however, stabilise the dynamics of the system, averaging out the effects of device mismatch. The three obstacle-representing populations are mutually inhibitory.



%\begin{figure}[h!]
%\begin{minipage}[b]{.5\linewidth}
%\centering\includegraphics[width=6cm]{YM-logo}% This is a *.eps file
%\subcaption{A subfigure}\label{fig:2a}
%\end{minipage}%
%\begin{minipage}[b]{.5\linewidth}
%\centering\includegraphics[width=2cm]{logo2}% This is an *.eps file
%\subcaption{Another subfigure}\label{fig:2b}
%\end{minipage}
%\caption{A figure}\label{fig:2b}
%\end{figure}





%- Robotic platform is the pushbot, cite TUM \& Inilabs  send events over tcp to parallela 
%- drop 80 \% of the events (no systematic noise, need better explanation)
%- 
%- c++ interface 


%The obstacle avoidance is implemented based on the Braitenberg vehicle principle. When the robot senses a large amount of events from objects on one side of the camera image, it turns in the opposite direction. Here, events from the lower half of the image are considered, which correspond to the closed-by objects. 



%\begin{figure}
%  \centering
%  \includegraphics[width=0.45\textwidth]{Figures/obstacles.pdf}
%  \caption{The link between the retinal space and the neuronal populations, representing direction to the most relevant obstacle.}
%  \label{fig:obstacles}
%\end{figure}


%Two pairs of neuronal populations represent the velocity of the left and right wheels of the robot in the forward and backward direction (we need four populations since the neuronal representations do not change sign). This populations are connected to the L-C-R obstacle representing populations in a way that realises avoidance behavior: left obstacle representing population excites the left wheel forward velocity population, right obstacle representing population excites the right wheel forward velocity population, whereas the center obstacle representing population excites both left and right wheel backward velocity populations.This leads to avoiding obstacles to the left and right from the robot and to a ``retreat'' maneuvre for a frontal obstacle or large clutter in front of the robot.  

%Each event is sent to the neuromorphic chip and is accumulated there. If enough events are accumulated, the motor command is sent to the robot to adjust its trajectory accordingly.

\subsubsection{Dynamic neural field for target representation}

To represent targets of the navigation dynamics, we use Dynamic Neural Fields (DNFs), as suggested in \citep{BichoMalletSchoner2000}. DNFs are population-based models of dynamics of large homogeneous neuronal populations, which were very successful in modelling elementary cognitive function in humans \citep{SchonerSpencer2015}, as well as in realisation of cognitive representations for robots \citep{Erlhagen2006,Bicho2011,SandamirskayaEtAl2013}. DNFs can be easily realised in neuromorphic hardware by setting a winner-take-all (WTA) connectivity network in a neural population. In a hardware WTA network, each neuron has a positive connection to itself and to its two or three nearest neighbours, implementing the lateral excitation of the DNF interaction kernel. Each neuron also has inhibitory connections to the rest of the WTA network, implementing global inhibition of a DNF. These inhibitory connections can be either direct, as used here, or be relayed through an inhibitory population, which is a more biologically plausible solution.   

In our architecture, we select 128 neurons on the ROLLS chip to represent visually perceived targets. Each neuron in this population receives events from one column of the 128x128 sensor frame from the eDVS and sums these events up according to its neuronal dynamics. The nearby neurons support each other's activation, while inhibiting further neurons in the WTA population. This connectivity stabilised localised blobs of most salient sensory events, filtering out sensor noise and objects that are too large (inhibition starts to play role within object representation) or too small (not enough lateral excitation is engaged).  Thus, the WTA connectivity stabilises the target representation. The target in our experiments was a blinking LED of the second robot, which is detected in the DNF, realised on the ROLLS. This target can be easily detected, more sophisticated vision algorithms are  required to pursue a less discriminative target \citep{CorradiEtAl2016}. In Fig.~\ref{fig:connectivity}, the DNF population is in the lower right corner of the connectivity matrix,  set on the ROLLS chip.


\subsubsection{Combining obstacle avoidance and target acquisition}

\begin{figure}
  \centering
  \includegraphics[width=0.45\textwidth]{Figures/connectivity_matrix_v3.pdf}
  \caption{The synaptic connectivity matrix, configured on the ROLLS chip to implement the obstacle avoidance and target acquisition arhcitectures. Colors encode different synaptic weights (red for positive and blue for negative connection weights).}
  \label{fig:connectivity}
\end{figure}


The two neuronal populations that drive robot's rotations  ($n_{DR}$ and $n_{DL}$) sum-up contributions from the obstacle-representing populations and the target-representing WTA population (Fig.~\ref{fig:NN_Arch}). The obstacle contribution is effectively stronger than the target contribution. Thus, in the presence of an obstacle in the robot's field of view, an obstacle avoidance manoeuvre is preferred. 

%The last position of the target representation in the target WTA is stored by the lateral excitatory connections in this neuronal population and guides the robot in the correct direction when the obstacle is left behind. 

Fig.~\ref{fig:connectivity} shows the connectivity matrix, used to configure the non-plastic connections on the ROLLS chip to realise both obstacle avoidance and target acquisition. This plot shows the weights of non-plastic synapses on the ROLLS chip (blue being the negative weights and red -- the positive weights, the same color code is used for the different weights as in Fig.~\ref{fig:NN_Arch}), which connect groups of neurons (different populations, labeled on the right side of the figure) among each there. Within-group connections are marked with black squared frames on the diagonal of the connectivity matrix. Violet and orange arrows show inputs and outputs of the architecture, respectively. 

 




%There are 5 heading levels

%\subsection{Level 2}
%\subsubsection{Level 3}
%\paragraph{Level 4}
%\subparagraph{Level 5}


%\begin{equation}
%\sum x+ y =Z\label{eq:01}
%\end{equation}




\section{Results}

We have tested performance of our system in two types of environment: first, we let the robot drive in a robotic arena with a controlled background and salient obstacles.  We used a tape with a contrastive texture to make the walls of the arena visible to the robot. We have tested our system in the presence of different number of obstacles, at different speeds, and with moving obstacles. We have also tested target acquisition in the arena. In the second set of experiments we put the robot in the office environment to verify that the reach background does not compromise the performance of the system. 

\subsection{Probing the obstacle avoidance: single obstacle studies}


Fig.~\ref{fig:basicObs} demonstrates how our neuronal architecture allows the robot to avoid an obstacle. On the left, an overlay of video frames (recording the top view of the arena) shows the robot's trajectory when avoiding a single obstacle (a cup) in the robotic arena. Numbers (1-3) mark important moments in time during the turning movement. On the right, activity of neuronal populations on the ROLLS device is shown. The moments in time, marked on the left figure are also marked in the time-plots on the right. Summed activity of the left and right obstacle populations (top plot) and left and right drive populations (middle plot) are shown. In this case, already the obstacle detecting populations had a clear ``winner'' -- the left population forms an increasing activity bump over time, which drives the ``drive right'' population, inducing a right turn of the robot. The bottom plot shows the commands that are sent to the robot (speed and angular velocity): the robot slows down in front of the obstacle and turns to the right.



\begin{figure}
  \centering
  \includegraphics[width=0.95\textwidth]{Figures/basic_obstacle_avoidance.png}
  \caption{An example of an obstacle avoidance maneuvre. Left: Overlay of video frames showing the trajectory of the robot. Right: activity of the neuronal populations on the chip (top: left and right obstacle detecting populations; bottom: left and right drive populations) and the motor commands, sent to the robot (bottom plot). }
  \label{fig:basicObs}
\end{figure}

We have performed a number of experiments with a single obstacle, varying the saliency of the obstacle, the robot's speed, lighting conditions, as well as parameters of the architecture. Since the architecture uses difference between the amount of sensory events in the two halves of the visual space, avoiding a single obstacle works robustly, although the camera might miss objects with a low contrast (e.g. yellow block in our white arena regularly provided not enough input to the obstacle avoidance system). More advanced noise filtering would improve performance here. 

%Table~\ref{tab:vels} shows the  dependence of the minimal distance between the robot and an obstacle depending on the speed factor (where factor 10 corresponds to the maximal speed of the robot). 
%
%
%\begin {table}
%\caption {Minimal distance to an obstacle depending on the velocity of the robot } \label{tab:vels} 
%\begin{center}
%  \begin{tabular}{ | l | c | c | c | c | c | c | c | }
%    \hline
%    Velocity &  0. 5 &  1.0  & 1.5 & 2.0 & 2.5 & 3.0 & 10 \\ \hline
%    Min. distance & 2.8 & 4.3 & 0.9 & 0.5 & 0.5 & 2.2 & 0  \\ 
%    \hline
%  \end{tabular}
%\end{center}
%\end{table}
 
\subsection{Avoiding multiple obstacles}
\label{sec:multipleObst}

Fig.~\ref{fig:twoObs} shows how the robot avoids a pair of obstacles. Snapshots from the video camera,  output of the DVS (with 80\% of events randomly dropped and the rest of events accumulated over 500ms),  and the activity of neuronal populations recorded from the ROLLS chip are shown. Activity is shown of the obstacle representing left (red) and right (blue) neuronal populations (third column), the left (red) and right (blue) drive populations, and the speed population (grey, forth column). Each of these populations has 16 neurons, dots represent their spikes.  The robot was moving with the speed factor 2. 


\begin{figure}
  \centering
  \includegraphics[width=0.95\textwidth]{Figures/two_blocks_20161115T123452_time_series_small.pdf}
  \caption{Avoiding a pair of obstacles. \textbf{First column}: Snapshots of four moments in time during avoidance of a cup, moved into the robot's trajectory. \textbf{Second column}: DVS events. Green dots are off events, blue dots are on events. Events in the upper part of the frame were not considered for the obstacle avoidance. \textbf{Third column}: Activity of the obstacle representing populations in 0.6-1.5 seconds before the camera snapshot in the first column was taken  (red -- left population ($n_{OL}$), blue -- right population ($n_OR$); each population has 16 neurons). \textbf{Forth column}: Activity of the drive left (red), drive right (blue), and speed population on the ROLLS chip in the same time as on the plots in column 3. }
  \label{fig:twoObs}
\end{figure}

At the moment, depicted in the top row of Fig.~\ref{fig:twoObs}, the robot senses an obstacle on the left, although the DVS output is rather weak. The robot turns right, driven by the activated drive right population and now the obstacle on the right becomes visible, providing a strong signal to the right obstacle population and, consequently, to the drive left population (second row). The robot turns to the left. Input coming from the second obstacle makes it turn right again to go between the obstacles (end of the time window in the third row), but the obstacle on the right overcomes and the robot drives past both obstacles on the left side (forth column).  

With our current parametrisation of the neuronal obstacle avoidance architecture, the robot has a tendency to go around a pair of objects, avoiding the space between them.  This behavior could be changed, adjusting the strength  of connections between the obstacle representing populations and drive populations. However, for a robot equipped with a DVS, such strategy is safer, since for homogeneous objects, the DVS can only sense the edges, where a temporal contrast change can be induced by the robot's motion. The robot thus might miss the central part of an object and avoiding pairs of close objects is a safer strategy. 

\begin{figure}
  \centering
  \includegraphics[width=0.95\textwidth]{Figures/random_parcour_20161121T191100.png}
  \caption{Navigation in a cluttered arena. \textbf{Left}: Overlayed frames from the video, recoding the robotic arena from the top. Green line markes the path of the robot.  \textbf{Right}: Summed activation of neurons in populations on the ROLLS chip over the time of the experiment. Obstacle and turn (left and right) population are shown, as well  as the commands sent to the robot (angular velocity and speed).}
  \label{fig:parcour}
\end{figure}

Fig.~\ref{fig:parcour} demonstrates behaviour of the obstacle avoidance system in a cluttered environment. In particular we let the robot drive in an arena, in which a large number of obstacles (either in the setting shown in the figure)  was randomly distributed. The robot successfully  avoids obstacles in its way with one exception: the robot touches the blue obstacle in the center of the arena, which was outside the field of view of the DVS camera. This points to a limitation of the current setup which uses single camera with a narrow field of view. Using multiple cameras would improve visibility of obstacles for the robot. Note, that we  used rather small objects in our experiments in the arena, which  pose a challenge for the DVS-based detection, especially taking into account our very simplistic noise-reduction strategy (dropping 80\% of events).  

\subsection{Variability of behavior}

Since behavior of our robot is controlled by activity of neuronal populations, implemented in analog neuromorphic hardware, the behavior of the robot has some variability, even when exactly the same parameters of the architecture and the same hardware biases are used. Despite this variability, the robot's goal -- avoiding obstacles -- is always fulfilled. Fig.~\ref{fig:var} demonstrates this behavior of our neuronal controller. 

\begin{figure}
  \centering
  \includegraphics[width=0.95\textwidth]{Figures/different_behaviour.png}
  \caption{Variability of the robot's behavior. \textbf{Left}: Overlay of video camera frames recording the robot, avoiding a pair of obstacles; top view. Three different trials are recorded and overlayed here (trajectories are shown with green lines 1-3). \textbf{Right}: Velocity commands, received by the robot from the neuronal architecture (angular velocity and speed) for the three trials (from top to bottom).  }
  \label{fig:var}
\end{figure}

In the figure, we show three trials, in which the robot avoids a two-blocks configuration, starting from exactly the same position and with the same configuration of the neuronal controller. Missmatch in the neuronal populations implemented in analog neuromorphic hardware, variability of the DVS output, and its dependence on the robot's movements lead to strong differences in trajectories. In particular, in the  case shown in Fig.~\ref{fig:var}, the trajectories may bifurcate and the robot might avoid the two obstacles on the right or on the left. Such variability of behavior, which, however, respects the important behavioral constraints (here, the task to avoid collisions) are a natural source of exploration, which may be exploited in learning scenarios in more complex architectures, built on top of our elementary obstacle avoidance system.



\subsection{Avoiding a moving obstacle}


Fig.~\ref{fig:movingObs} shows how the robot avoids a moving obstacle (a cup, moved by hand into the robot's path). The same arrangement of plots was used as in Fig.~\ref{fig:twoObs}, described in Section~\ref{sec:multipleObst}.  The robot was moving with xx speed, the cup was moved at approx. 10cm/s. 
 
\begin{figure}
  \centering
  \includegraphics[width=0.95\textwidth]{Figures/moving_cup_05_20161115T175136_time_series_small.pdf}
  \caption{Avoiding a moving obstacle. The same arrangement is used as in Fig.~\ref{fig:twoObs}. See main text for the discussion.}
  \label{fig:movingObs}
\end{figure}

First, the cup is outside the view of the robot's DVS and both obstacle representing and drive pairs of neuronal populations  are silent (no spikes). The speed population is active and drives the forward motion of the robot (first row in Fig.~\ref{fig:movingObs}). Second, the cup becomes visible in the robot's DVS view (a few events in the lower half of the frame in Fig.~\ref{fig:movingObs}, second row and column). The left obstacle population $n_{OL}$ gets activated (first spike appears around 3.3s, second row and third column in the figure) and drives the drive right population (second row, forth column). The robot starts turning to the right (Fig.~\ref{fig:movingObs}, second row). In the third row of Fig.~\ref{fig:movingObs}, the robot continues turning as the moving cup is still in its view, the left obstacle population and the right drive population are firing, inhibiting the speed population. Finally, after 6 seconds, the robot turned far enough so that the cup is no longer in its view, the obstacle population and the drive population stop firing, releasing inhibition on the speed population, the robot speeds up again. 


We have tested the robot with a number of moving obstacles at different speeds, showing that the robot is capable to avoid collisions in most cases. Videos in the supplementary material show examples of successful and unsuccessful trials. 

\subsection{Obstacle-avoidance in a real-world environment}

Fig.~\ref{fig:driving_office} shows an example of the Pushbot robot driving in the office environment. 


\begin{figure}
  \centering
  \includegraphics[width=0.95\textwidth]{Figures/3_timepoints_cam_edvs_rolls.pdf}
  \caption{Robot driving in the office environment. \textbf{Left}: Snapshots from the video camera showing robot at three time points during the experiment. \textbf{Middle}: Events from the DVS camera and histogram of these events, binned over 500ms in columns in the region between two vertical lines, which were used to drive obstacle populations on the ROLLS. Each pare of the eDVS events and histogram corresponds to the time point of the video frame in the \textbf{Left} column. Note that 80\% of events are randomly dropped here and only ``on'' events are shown in the relevant region (lower part of the screen). Events above the midline of the image sensor are shown with transparency (these events were not used for obstacle avoidance). \textbf{Right}: Activity of the obstacle (left and right), drive (left and right), and speed neuronal populations over time (summed activity across each population). Vertical lines mark time point that correspond to the video frames in the \textbf{Left} column.}
  \label{fig:driving_office}
\end{figure}

On the left, three snapshots from the video camera recording the roaring robot are shown (full videos can be see in the Supplementary material). The snapshots show the robot navigating the office environment with its  task being to avoid collisions. The middle column of plots shows pairs of eDVS events, accumulated over 500ms around the moment in time in the corresponding snapshot on the left. 	Events above the midline of the eDVS field of view are shown with transparency to emphasise that they were not used for obstacle avoidance: only events from the region of the eDVS field-of-view between the two vertical lines in Fig.~\ref{fig:driving_office} were used. 

Histograms below the eDVS plots show histograms of the events from this region of the field of view, summed over the eDVS columns. These events drive the obstacle left (red colored part of the histogram) and obstacle right (blue part of the histogram) neuronal populations on the ROLLS chip. 

The right column shows activity of the neuronal populations on the ROLLS chip over time. Activity is calculated as a number of spikes emitted by  neurons in a population in a 500ms time-windows. Activity of the obstacle populations (right and left) is shown in the top plot, of the drive (turn) populations (right and left) is shown in the middle plot, whereas the bottom plot shows activity of the speed population. Black vertical lines mark time moments that correspond to the three snapshots in the left column. These plots allow to see that although the left and right obstacle populations are often activated concurrently, only one of the drive populations (either left or right) is active at any moment, leading to a clear decision to turn in either direction in the presence of perceived obstacles. The speed plot shows that movement of the robot is not very smooth -- it slows down and accelerates often based on the sensed presence of obstacles. This will be improved in future implementations of the model.    

When driving around the office, robot faced very different lighting conditions, as can be seen already in the three snapshots presented here. This variation in lighting conditions did not effect obstacle avoidance, since the DVS is sensitive to relative change of pixel's intensity, which varies less than absolute intensity at changing amount of ambient light.

 
 \subsection{Target acquisition}
 
 Here, we show a proof of concept demonstration for target acquisition. Obviously, the simple visual preprocessing that we use here does not allow us to distinguish target from obstacles (other than through their position in the upper or lower part of the field of view of the DVS). Moreover, we would need an object detection algorithm to detect the target and segregate it from background. This vision processing is outside the scope of our work, but there's a multitude of studies going in this direction \citep{Corradi2015} using morden deep/convolutional neural networks learning techniques.
 
 Fig. shows target acquisition for a static target  ad demonstrates that the robot can approach the target object. At a short distance, the obstacle component takes over and the robots turns away after approaching the target. 
 
 \begin{figure}
  \centering
  \includegraphics[width=0.95\textwidth]{Figures/follow_target_2_20161124T150619_time_series_small.pdf}
  \caption{Chasing a moving target. \textbf{First row}:  Snapshots from the overhead camera showing the robot controlled by the ROLLS chip chasing the manually contorlled robot. \textbf{Second row}: Summed eDVS events from 500ms time windows around the same moments. Events from the upper part were used for target acquisition, events from the lower part -- for obstacle avoidance. \textbf{Third row}: Spiking activity of the target WTA population on the ROLLS chip over the time of the experiment. Vertical line show the moments in time, selected for the first two rows. \textbf{Forth row}: Activity of the left and right obstacle populations (summed over 16 neurons of the populations and over 500ms time window).  \textbf{Fifth row}: Activity of the left and right drive (turn) populations (obtained in the same manner).} 
  \label{fig:tar_move}
\end{figure}

 
 Fig.~\ref{fig:tar_move} shows how the robot can chase a moving taget. We have controlled the second Pushbot remotely and have turned its LED on (at 100Hz). The LED provided a rather strong (though spatially very small) input to the DVS of the second, autonomously navigating robot. This input was integrated by our target WTA (DNF) population, which, however, also received a large amount of input from the background. Input from the localised LED was stronger and more concise, so that the respective location in the target WTA formed a ``winner'' (localised activity bump in the DNF terminology) and inhibited the interfering inputs from other locations. 
 
 In the figure, four snapshots of the video recording the two robots are shown (top row). The leading robot was covered with white paper to reduce interference from the obstacle avoidance dynamics as the robots get close to each other (the space in the arena and the small size of the blinking LED forced us to put the robots rather close to each other, so that the target robot could be occasionally  perceived as an obstacle). In the second row in Fig.~\ref{fig:tar_move}, the summed over 500ms events of the DVS are shown, around the same time points as the snapshots. Only the upper part of the field-of-view was considered for target acquisition. This part is very noisy, since the robot ``sees'' outside the arena and perceives objects in the background, which made target acquisition very challenging. Still, the blinking LED provided the strongest input and in most cases the target DNF was able to select its input as the target and suppress the competing inputs from the background -- see activity of neurons in the target DNF in the third row. In this plot that shows spiking activity of the 128 neurons of the target DNF (WTA), we can see that the system successfully selects the correct target in most cases, only loosing it from sight twice, as the robot receives particularly many DVS events from the background during turning. The forth and the fifth rows in the figure show, in the usual manner, the activity of the obstacle and drive neuronal populations.      
 
\section{Discussion}

This paper presented a neuronal architecture for reactive obstacle avoidance and target acquisition, implemented on an analog neuromorphic device ROLLS \citep{Qiao2015} and using a neuromorphic camera DVS as the only source of information about the environment. We have demonstrated  that the robot, controlled by interconnected populations of artificial spiking neurons, is capable to avoid multiple, as well as moving obstacles at its maximal speed. We have also demonstrated that the systems works in a real-world office environment, where background clutter poses a challenge for the DVS on a moving vehicle, creating many distracting events. 

Certainly, this very simple system has a number of limitations, which we will address in our future work. Thus, ...


\section{References}
%\bibliographystyle{plain}
\bibliographystyle{frontiersinSCNS_ENG_HUMS} % for Science, Engineering and Humanities and Social Sciences articles, for Humanities and Social Sciences articles please include page numbers in the in-text citations
%\bibliographystyle{frontiersinHLTH&FPHY} % for Health, Physics and Mathematics articles
\bibliography{literature_mendeley2}

\section{Appendix}
\label{sec:appendix}
\subsection{Neuronal equations}
\label{sec:appendix1}
The dynamics of the spiking neurons on the ROLLS chip can be approximated by the differential equation Eqs.~(\ref{eq:IF1}-\ref{eq:IF3}), obtained by performing circuit analysis:

\begin{align}
\tau \frac{dImem}{dt}&=\frac{\frac{I_{th}}{I_\tau}(I_{in}-I_{ahp}-I_\tau)+\frac{I_a}{I_\tau}(I_{mem}+I_{th})-I_{mem}(1+\frac{I_{ahp}}{I_\tau})}{(1+\frac{I_{th}}{I_{mem}})} \label{eq:IF1}\\
\tau_{ahp} \frac{dI_{ahp}}{dt}&=\frac{I_{thahp}}{I_{\tau ahp}}I_{Ca}u(t)-I_{ahp}
\label{eq:IF2}
\end{align}
where $I_{mem}$ is the membrane potential, $I_{ahp}$ is the adaptation current, $u(t)$ is a step function that is one during spikes and zero otherwise, $I_\tau$ and $I_{\tau ahp}$ are time constant currents, $I_th$ and $I_{thahp}$ are currents through N-type MOSFETs, $\tau$ and $\tau_{ahp}$ are time constants, and $I_{in}$ is the input current from the synapses.\par
The time constants are dependent on the time constant currents and can be calculated by:
\begin{equation}
\tau = \frac{C_{mem}U_T}{\kappa I_{tau}}
\label{eq:IF3}
\end{equation}
where $\kappa$ is a MOSFET property, $U_T$ is the thermal potential, and $C_{mem}$ is the membrane capacitance. $\tau_{ahp}$ is calculated similarly except it substitutes $I_\tau$ with $I_{\tau ahp}$ and $C_{mem}$ with $C_p$.\par

These equations approximate an adaptive  integrate-and-fire dynamics \citep{BretteGerstner2005} . 


\subsection{Biases of the ROLLS chip used in our experiments}
\label{sec:appendix2}
Table~\ref{tab:biases} shows the biases used for our experiments to set-up non-plastic connections between the neuronal populations; Table~\ref{tab:IF} shows biases for the integrate-and-fire neurons on chip. Each bias corresponds to a current, supplied to the neuronal circuits and is calculated as Range $\times$ Value, where letters near the range mean the order of magnitude: ``p'' -- piko, ``n'' -- nano, ``u'' -- micro (see \citep{Qiao2015} for details of the circuit and functional meaning of the biases). The biases are set using software and FPGA-mapping, implemented on the parallella board. 

\begin {table}
\caption {Hardware biases for the  non-plastic synapses} \label{tab:biases} 
\begin{center}
  \begin{tabular}{ | l | c | c | l | }
    \hline
    Bias name &  Range (A) & Value & Flags\\ \hline
    NPA\_PWLK\_P & 820p & 200 & H  \\ 
    NPA\_WEIGHT\_STD\_N & 15p & 15 & H N  \\ 
    NPA\_WEIGHT\_EXC\_P & 820p & 123 & H  \\ 
    NPA\_WEIGHT\_EXC0\_P & 0.4u & 15 & H  \\ 
    NPA\_WEIGHT\_EXC1\_P & 0.4u & 82 & H  \\ 
    NPDPIE\_THR\_P & 820p & 38 & H  \\ 
    NPDPIE\_TAU\_P & 105p & 22 & H  \\ 
    NPA\_WEIGHT\_INH\_N & 820p & 82 & H N \\ 
    NPA\_WEIGHT\_INH\_N & 820p & 200 & H N \\ 
    NPA\_WEIGHT\_INH\_N & 6.5n & 71& H N \\ 
    NPDPII\_TAU\_P & 15p & 51 & H  \\ 
    NPDPII\_THR\_P  & 820p & 177 & H  \\
    \hline
  \end{tabular}
\end{center}
\end{table}

\begin {table}
\caption {Hardware biases for integrate-and-fire neurons} \label{tab:IF} 
\begin{center}
  \begin{tabular}{ | l | c | c | l | }
    \hline
    Bias name &  Range (A) & Value & Flags\\ \hline
    IF\_RST\_N & 15p & 17 & H N  \\ 
    IF\_BUR\_P & 50p & 56 & H N  \\ 
    IF\_ATHR\_N & 15p & 0 & H  N \\ 
    IF\_RFR1\_N & 820p & 50 & H  N \\ 
    IF\_RFR2\_N & 820p & 50 & H  N \\ 
    IF\_AHW\_P & 15p & 0 & H  \\ 
    IF\_AHTAU\_N & 820p & 37 & \hspace{0.32cm} N  \\ 
    IF\_DC\_P & 15p & 0 & H  \\ 
    IF\_TAU2\_N & 105p & 77 & \hspace{0.32cm} N \\ 
    IF\_TAU1\_N & 105p & 100 & \hspace{0.32cm}  N \\ 
    IF\_NMDA\_N & 15p & 17 & H N  \\ 
    IF\_CASC\_N & 15p & 17 & H  N \\
     IF\_THR\_N & 820p & 100 & H  N \\
    \hline
  \end{tabular}
\end{center}
\end{table}




\section*{Conflict of Interest Statement}
%All financial, commercial or other relationships that might be perceived by the academic community as representing a potential conflict of interest must be disclosed. If no such relationship exists, authors will be asked to confirm the following statement: 

The authors declare that the research was conducted in the absence of any commercial or financial relationships that could be construed as a potential conflict of interest.

\section*{Author Contributions}

MM conceptualisation of the model, analyses of the results, writing up;
HB implementation of combined obstacles avoidance and target acquisition, experiments, results analysis, writing up;
AD implementation of combined obstacles avoidance and target acquisition, experiments, results analysis, writing up;
DS implementation of first version of obstacle avoidance, parameter tuning on the chip, state of the art analysis;
JC support with robotic hardware and middleware, analysis of the results, writing up;
GI support with neuromorphic hardware, state of the art and result analysis, writing up;
YS conceptualisation of the model, development of the architecture, experiments design, analysis of the results, embedding in the literature, discussion of the results, writing, overall supervision of the project.


\section*{Funding}
Supported by EU H2020-MSCA-IF-2015 grant 707373 ECogNet and EU ERC-2010-StG 20091028 grant 257219 NeuroP, as well as INIForum.

\section*{Acknowledgments}
We would like to thank Aleksander Kazhdabaev and Julien Martel for their help with the software code used in this work. This work has started at the Capo Caccia 2016 Workshop for Neuromorphic Engineering.

\section*{Supplemental Data}
 \href{http://home.frontiersin.org/about/author-guidelines#SupplementaryMaterial}{Supplementary Material} includes videos of our robotic experiments.



%%% Upload the *bib file along with the *tex file and PDF on submission if the bibliography is not in the main *tex file

%%% Please see the test.bib file for some examples of references

%\section*{Figures}

%%% Use this if adding the figures directly in the mansucript, if so, please remember to also upload the files when submitting your article
%%% There is no need for adding the file termination, as long as you indicate where the file is saved. In the examples below the files (logo1.jpg and logo2.eps) are in the Frontiers LaTeX folder
%%% If using *.tif files convert them to .jpg or .png
%%% If using panelled/compiled figures with subcaptions, use the subcaption package as in the second example.  N.B. This package is incompatible with the subfigure and subfig packages
%%%  NB logo1.jpg is required in the path in order to correctly compile front page header %%%

%\begin{figure}[h!]
%\begin{center}
%\includegraphics[width=10cm]{logo1}% This is a *.jpg file
%\end{center}
%\caption{ Enter the caption for your figure here.  Repeat as  necessary for each of your figures}\label{fig:1}
%\end{figure}
%
%\begin{figure}[h!]
%\begin{minipage}[b]{.5\linewidth}
%\centering\includegraphics[width=6cm]{YM-logo}% This is a *.eps file
%\subcaption{A subfigure}\label{fig:2a}
%\end{minipage}%
%\begin{minipage}[b]{.5\linewidth}
%\centering\includegraphics[width=2cm]{logo2}% This is an *.eps file
%\subcaption{Another subfigure}\label{fig:2b}
%\end{minipage}
%\caption{A figure}\label{fig:2b}
%\end{figure}


%%% If you don't add the figures in the LaTeX files, please upload them when submitting the article.

%%% Frontiers will add the figures at the end of the provisional pdf automatically %%%

%%% The use of LaTeX coding to draw Diagrams/Figures/Structures should be avoided. They should be external callouts including graphics.

\end{document}


\end{document}

-----END DOCUMENT-----

Log file:
-----BEGIN LOG-----
This is pdfTeX, Version 3.14159265-2.6-1.40.16 (TeX Live 2015/Debian) (preloaded format=pdflatex 2016.7.15)  25 NOV 2016 11:16
entering extended mode
 restricted \write18 enabled.
 %&-line parsing enabled.
**5de382f01d62152f71976f700c023332.tex
(./5de382f01d62152f71976f700c023332.tex
LaTeX2e <2016/02/01>
Babel <3.9q> and hyphenation patterns for 5 language(s) loaded.
(/usr/share/texlive/texmf-dist/tex/latex/standalone/standalone.cls
Document Class: standalone 2015/07/15 v1.2 Class to compile TeX sub-files stand
alone
(/usr/share/texlive/texmf-dist/tex/generic/oberdiek/ifluatex.sty
Package: ifluatex 2010/03/01 v1.3 Provides the ifluatex switch (HO)
Package ifluatex Info: LuaTeX not detected.
)
(/usr/share/texlive/texmf-dist/tex/generic/oberdiek/ifpdf.sty
Package: ifpdf 2011/01/30 v2.3 Provides the ifpdf switch (HO)
Package ifpdf Info: pdfTeX in PDF mode is detected.
)
(/usr/share/texlive/texmf-dist/tex/generic/ifxetex/ifxetex.sty
Package: ifxetex 2010/09/12 v0.6 Provides ifxetex conditional
)
(/usr/share/texlive/texmf-dist/tex/latex/xkeyval/xkeyval.sty
Package: xkeyval 2014/12/03 v2.7a package option processing (HA)

(/usr/share/texlive/texmf-dist/tex/generic/xkeyval/xkeyval.tex
(/usr/share/texlive/texmf-dist/tex/generic/xkeyval/xkvutils.tex
\XKV@toks=\toks14
\XKV@tempa@toks=\toks15

(/usr/share/texlive/texmf-dist/tex/generic/xkeyval/keyval.tex))
\XKV@depth=\count79
File: xkeyval.tex 2014/12/03 v2.7a key=value parser (HA)
))
\sa@internal=\count80
\c@sapage=\count81

(/usr/share/texlive/texmf-dist/tex/latex/standalone/standalone.cfg
File: standalone.cfg 2015/07/15 v1.2 Default configuration file for 'standalone
' class
)
(/usr/share/texlive/texmf-dist/tex/latex/base/article.cls
Document Class: article 2014/09/29 v1.4h Standard LaTeX document class
(/usr/share/texlive/texmf-dist/tex/latex/base/size10.clo
File: size10.clo 2014/09/29 v1.4h Standard LaTeX file (size option)
)
\c@part=\count82
\c@section=\count83
\c@subsection=\count84
\c@subsubsection=\count85
\c@paragraph=\count86
\c@subparagraph=\count87
\c@figure=\count88
\c@table=\count89
\abovecaptionskip=\skip41
\belowcaptionskip=\skip42
\bibindent=\dimen102
)
(/usr/share/texmf/tex/latex/preview/preview.sty
Package: preview 2010/02/14 11.88 (AUCTeX/preview-latex)

(/usr/share/texmf/tex/latex/preview/prtightpage.def
\PreviewBorder=\dimen103
)
\pr@snippet=\count90
\pr@box=\box26
\pr@output=\toks16
))
(/usr/share/texlive/texmf-dist/tex/latex/xcolor/xcolor.sty
Package: xcolor 2007/01/21 v2.11 LaTeX color extensions (UK)

(/usr/share/texlive/texmf-dist/tex/latex/latexconfig/color.cfg
File: color.cfg 2007/01/18 v1.5 color configuration of teTeX/TeXLive
)
Package xcolor Info: Driver file: pdftex.def on input line 225.

(/usr/share/texlive/texmf-dist/tex/latex/pdftex-def/pdftex.def
File: pdftex.def 2011/05/27 v0.06d Graphics/color for pdfTeX

(/usr/share/texlive/texmf-dist/tex/generic/oberdiek/infwarerr.sty
Package: infwarerr 2010/04/08 v1.3 Providing info/warning/error messages (HO)
)
(/usr/share/texlive/texmf-dist/tex/generic/oberdiek/ltxcmds.sty
Package: ltxcmds 2011/11/09 v1.22 LaTeX kernel commands for general use (HO)
)
\Gread@gobject=\count91
)
Package xcolor Info: Model `cmy' substituted by `cmy0' on input line 1337.
Package xcolor Info: Model `hsb' substituted by `rgb' on input line 1341.
Package xcolor Info: Model `RGB' extended on input line 1353.
Package xcolor Info: Model `HTML' substituted by `rgb' on input line 1355.
Package xcolor Info: Model `Hsb' substituted by `hsb' on input line 1356.
Package xcolor Info: Model `tHsb' substituted by `hsb' on input line 1357.
Package xcolor Info: Model `HSB' substituted by `hsb' on input line 1358.
Package xcolor Info: Model `Gray' substituted by `gray' on input line 1359.
Package xcolor Info: Model `wave' substituted by `hsb' on input line 1360.
)
(/usr/share/texlive/texmf-dist/tex/latex/amsmath/amsmath.sty
Package: amsmath 2016/03/03 v2.15a AMS math features
\@mathmargin=\skip43

For additional information on amsmath, use the `?' option.
(/usr/share/texlive/texmf-dist/tex/latex/amsmath/amstext.sty
Package: amstext 2000/06/29 v2.01 AMS text

(/usr/share/texlive/texmf-dist/tex/latex/amsmath/amsgen.sty
File: amsgen.sty 1999/11/30 v2.0 generic functions
\@emptytoks=\toks17
\ex@=\dimen104
))
(/usr/share/texlive/texmf-dist/tex/latex/amsmath/amsbsy.sty
Package: amsbsy 1999/11/29 v1.2d Bold Symbols
\pmbraise@=\dimen105
)
(/usr/share/texlive/texmf-dist/tex/latex/amsmath/amsopn.sty
Package: amsopn 1999/12/14 v2.01 operator names
)
\inf@bad=\count92
LaTeX Info: Redefining \frac on input line 199.
\uproot@=\count93
\leftroot@=\count94
LaTeX Info: Redefining \overline on input line 297.
\classnum@=\count95
\DOTSCASE@=\count96
LaTeX Info: Redefining \ldots on input line 394.
LaTeX Info: Redefining \dots on input line 397.
LaTeX Info: Redefining \cdots on input line 518.
\Mathstrutbox@=\box27
\strutbox@=\box28
\big@size=\dimen106
LaTeX Font Info:    Redeclaring font encoding OML on input line 630.
LaTeX Font Info:    Redeclaring font encoding OMS on input line 631.
\macc@depth=\count97
\c@MaxMatrixCols=\count98
\dotsspace@=\muskip10
\c@parentequation=\count99
\dspbrk@lvl=\count100
\tag@help=\toks18
\row@=\count101
\column@=\count102
\maxfields@=\count103
\andhelp@=\toks19
\eqnshift@=\dimen107
\alignsep@=\dimen108
\tagshift@=\dimen109
\tagwidth@=\dimen110
\totwidth@=\dimen111
\lineht@=\dimen112
\@envbody=\toks20
\multlinegap=\skip44
\multlinetaggap=\skip45
\mathdisplay@stack=\toks21
LaTeX Info: Redefining \[ on input line 2735.
LaTeX Info: Redefining \] on input line 2736.
)
(/usr/share/texlive/texmf-dist/tex/latex/amsfonts/amssymb.sty
Package: amssymb 2013/01/14 v3.01 AMS font symbols

(/usr/share/texlive/texmf-dist/tex/latex/amsfonts/amsfonts.sty
Package: amsfonts 2013/01/14 v3.01 Basic AMSFonts support
\symAMSa=\mathgroup4
\symAMSb=\mathgroup5
LaTeX Font Info:    Overwriting math alphabet `\mathfrak' in version `bold'
(Font)                  U/euf/m/n --> U/euf/b/n on input line 106.
))
(/usr/share/texlive/texmf-dist/tex/latex/base/latexsym.sty
Package: latexsym 1998/08/17 v2.2e Standard LaTeX package (lasy symbols)
\symlasy=\mathgroup6
LaTeX Font Info:    Overwriting symbol font `lasy' in version `bold'
(Font)                  U/lasy/m/n --> U/lasy/b/n on input line 52.
)
(/usr/share/texlive/texmf-dist/tex/latex/mathtools/mathtools.sty
Package: mathtools 2015/11/12 v1.18 mathematical typesetting tools

(/usr/share/texlive/texmf-dist/tex/latex/tools/calc.sty
Package: calc 2014/10/28 v4.3 Infix arithmetic (KKT,FJ)
\calc@Acount=\count104
\calc@Bcount=\count105
\calc@Adimen=\dimen113
\calc@Bdimen=\dimen114
\calc@Askip=\skip46
\calc@Bskip=\skip47
LaTeX Info: Redefining \setlength on input line 80.
LaTeX Info: Redefining \addtolength on input line 81.
\calc@Ccount=\count106
\calc@Cskip=\skip48
)
(/usr/share/texlive/texmf-dist/tex/latex/mathtools/mhsetup.sty
Package: mhsetup 2010/01/21 v1.2a programming setup (MH)
)
LaTeX Info: Thecontrolsequence`\('isalreadyrobust on input line 129.
LaTeX Info: Thecontrolsequence`\)'isalreadyrobust on input line 129.
LaTeX Info: Thecontrolsequence`\['isalreadyrobust on input line 129.
LaTeX Info: Thecontrolsequence`\]'isalreadyrobust on input line 129.
\g_MT_multlinerow_int=\count107
\l_MT_multwidth_dim=\dimen115
\origjot=\skip49
\l_MT_shortvdotswithinadjustabove_dim=\dimen116
\l_MT_shortvdotswithinadjustbelow_dim=\dimen117
\l_MT_above_intertext_sep=\dimen118
\l_MT_below_intertext_sep=\dimen119
\l_MT_above_shortintertext_sep=\dimen120
\l_MT_below_shortintertext_sep=\dimen121
)
No file 5de382f01d62152f71976f700c023332.aux.
\openout1 = `5de382f01d62152f71976f700c023332.aux'.

LaTeX Font Info:    Checking defaults for OML/cmm/m/it on input line 10.
LaTeX Font Info:    ... okay on input line 10.
LaTeX Font Info:    Checking defaults for T1/cmr/m/n on input line 10.
LaTeX Font Info:    ... okay on input line 10.
LaTeX Font Info:    Checking defaults for OT1/cmr/m/n on input line 10.
LaTeX Font Info:    ... okay on input line 10.
LaTeX Font Info:    Checking defaults for OMS/cmsy/m/n on input line 10.
LaTeX Font Info:    ... okay on input line 10.
LaTeX Font Info:    Checking defaults for OMX/cmex/m/n on input line 10.
LaTeX Font Info:    ... okay on input line 10.
LaTeX Font Info:    Checking defaults for U/cmr/m/n on input line 10.
LaTeX Font Info:    ... okay on input line 10.
Preview: Fontsize 10pt
Preview: PDFoutput 1
(/usr/share/texlive/texmf-dist/tex/context/base/supp-pdf.mkii
[Loading MPS to PDF converter (version 2006.09.02).]
\scratchcounter=\count108
\scratchdimen=\dimen122
\scratchbox=\box29
\nofMPsegments=\count109
\nofMParguments=\count110
\everyMPshowfont=\toks22
\MPscratchCnt=\count111
\MPscratchDim=\dimen123
\MPnumerator=\count112
\makeMPintoPDFobject=\count113
\everyMPtoPDFconversion=\toks23
) (/usr/share/texlive/texmf-dist/tex/latex/graphics/graphicx.sty
Package: graphicx 2014/10/28 v1.0g Enhanced LaTeX Graphics (DPC,SPQR)

(/usr/share/texlive/texmf-dist/tex/latex/graphics/graphics.sty
Package: graphics 2016/01/03 v1.0q Standard LaTeX Graphics (DPC,SPQR)

(/usr/share/texlive/texmf-dist/tex/latex/graphics/trig.sty
Package: trig 2016/01/03 v1.10 sin cos tan (DPC)
)
(/usr/share/texlive/texmf-dist/tex/latex/latexconfig/graphics.cfg
File: graphics.cfg 2010/04/23 v1.9 graphics configuration of TeX Live
)
Package graphics Info: Driver file: pdftex.def on input line 95.
)
(/usr/share/texlive/texmf-dist/tex/generic/oberdiek/pdftexcmds.sty
Package: pdftexcmds 2011/11/29 v0.20 Utility functions of pdfTeX for LuaTeX (HO
)
Package pdftexcmds Info: LuaTeX not detected.
Package pdftexcmds Info: \pdf@primitive is available.
Package pdftexcmds Info: \pdf@ifprimitive is available.
Package pdftexcmds Info: \pdfdraftmode found.
)
(/usr/share/texlive/texmf-dist/tex/latex/oberdiek/epstopdf-base.sty
Package: epstopdf-base 2010/02/09 v2.5 Base part for package epstopdf

(/usr/share/texlive/texmf-dist/tex/latex/oberdiek/grfext.sty
Package: grfext 2010/08/19 v1.1 Manage graphics extensions (HO)

(/usr/share/texlive/texmf-dist/tex/generic/oberdiek/kvdefinekeys.sty
Package: kvdefinekeys 2011/04/07 v1.3 Define keys (HO)
))
(/usr/share/texlive/texmf-dist/tex/latex/oberdiek/kvoptions.sty
Package: kvoptions 2011/06/30 v3.11 Key value format for package options (HO)

(/usr/share/texlive/texmf-dist/tex/generic/oberdiek/kvsetkeys.sty
Package: kvsetkeys 2012/04/25 v1.16 Key value parser (HO)

(/usr/share/texlive/texmf-dist/tex/generic/oberdiek/etexcmds.sty
Package: etexcmds 2011/02/16 v1.5 Avoid name clashes with e-TeX commands (HO)
Package etexcmds Info: Could not find \expanded.
(etexcmds)             That can mean that you are not using pdfTeX 1.50 or
(etexcmds)             that some package has redefined \expanded.
(etexcmds)             In the latter case, load this package earlier.
)))
Package grfext Info: Graphics extension search list:
(grfext)             [.png,.pdf,.jpg,.mps,.jpeg,.jbig2,.jb2,.PNG,.PDF,.JPG,.JPE
G,.JBIG2,.JB2,.eps]
(grfext)             \AppendGraphicsExtensions on input line 452.

(/usr/share/texlive/texmf-dist/tex/latex/latexconfig/epstopdf-sys.cfg
File: epstopdf-sys.cfg 2010/07/13 v1.3 Configuration of (r)epstopdf for TeX Liv
e
))
\Gin@req@height=\dimen124
\Gin@req@width=\dimen125
)
LaTeX Font Info:    Try loading font information for U+msa on input line 13.

(/usr/share/texlive/texmf-dist/tex/latex/amsfonts/umsa.fd
File: umsa.fd 2013/01/14 v3.01 AMS symbols A
)
LaTeX Font Info:    Try loading font information for U+msb on input line 13.

(/usr/share/texlive/texmf-dist/tex/latex/amsfonts/umsb.fd
File: umsb.fd 2013/01/14 v3.01 AMS symbols B
)
LaTeX Font Info:    Try loading font information for U+lasy on input line 13.

(/usr/share/texlive/texmf-dist/tex/latex/base/ulasy.fd
File: ulasy.fd 1998/08/17 v2.2e LaTeX symbol font definitions
))
Runaway argument?
{e = (x,\ y,\ ts,\ p), $ where $x$ and $y$ define the pixel location \ETC.
! File ended while scanning use of \textbf .
<inserted text> 
                \par 
<*> 5de382f01d62152f71976f700c023332.tex
                                        
I suspect you have forgotten a `}', causing me
to read past where you wanted me to stop.
I'll try to recover; but if the error is serious,
you'd better type `E' or `X' now and fix your file.

! Emergency stop.
<*> 5de382f01d62152f71976f700c023332.tex
                                        
*** (job aborted, no legal \end found)

 
Here is how much of TeX's memory you used:
 3917 strings out of 494910
 55262 string characters out of 6179836
 137815 words of memory out of 5000000
 7148 multiletter control sequences out of 15000+600000
 5657 words of font info for 25 fonts, out of 8000000 for 9000
 36 hyphenation exceptions out of 8191
 51i,4n,56p,1518b,134s stack positions out of 5000i,500n,10000p,200000b,80000s
!  ==> Fatal error occurred, no output PDF file produced!

-----END LOG-----