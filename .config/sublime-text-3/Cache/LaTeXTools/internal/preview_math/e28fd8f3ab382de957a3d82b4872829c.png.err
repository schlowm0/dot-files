Failed to run 'pdflatex' to create pdf to preview.


Errors:
TeX STOPPED: File ended while scanning use of \frac{(1 - \phi ), \end {equation} where $\tau _{pred}$ is the time consta
TeX reports the error was in file: e28fd8f3ab382de957a3d82b4872829c.tex

LaTeX document:
-----BEGIN DOCUMENT-----

\documentclass[preview,border=0.3pt]{standalone}
% import xcolor if available and not already present
\IfFileExists{xcolor.sty}{\usepackage{xcolor}}{}%
\usepackage{amsmath}
\usepackage{amssymb}
\IfFileExists{latexsym.sty}{\usepackage{latexsym}}{}
\IfFileExists{mathtools.sty}{\usepackage{mathtools}}{}

\begin{document}
% set the foreground color
\IfFileExists{xcolor.sty}{\color[HTML]{A6B2C0}}{}%
\(
\label{eq:ase_dyn}
    \frac{d\phi}{dt} = \frac{(1 - \phi),
\end{equation}
where $\tau_{pred}$ is the time constant with which the synaptic gain value converges back to 1 (see Figure~\ref{fig:ase_octa}).
\subsection{Additional mechanism for learning}
\label{sec:remarks_learning}
\paragraph{Device mismatch}
Neuromorphic processors are subject to device mismatch \citep[see Section~\ref{sec:ebprocessors} \&][]{Pelgrom_etal89}.
An algorithm emulated on these processors needs to to be capable of either compensating device mismatch (see Chapter~\ref{ch:reactivecontrol}) or adjust its parameters during training to cope with device mismatch which is characterised by heterogeneous response profiles of its processing elements.
To simulate an element-to-element variability of approximately 20\% we draw each parameter from a truncated normal distribution with a standard deviation of 0.2.

\paragraph{Weight decay}
All synaptic weights which are subject learning are also subject to a constant weight decay\footnote{Constant weight decay is a common practice in computer simulations \citep[e.g.\ ][]{Krogh_Hertz92,Krizhevsky_etal12}. and can be also observed in biological synapses \citep{Cui_etal04,Abraham_etal05}}.
We decay the weight by a factor proportional to the associated learning rate $\eta$, i.e. $1 - (\eta / 10)$.
This weight decay prevents synaptic weights from becoming too correlated which would cause synchronous and oscillatory network activity.
Synchronous network activity can be understood as that the network's output is independent of the input, thus indicating that no information is being encoded in the ongoing activity of the network and no computation is being performed.

\paragraph{Weight re-initialisation}
Inhibition provided by the interneurons can prevent neurons from being active.
In combination with a constant weight decay this can lead a reduction of synaptic weights to zero.
To ensure that the constant weight decay does not introduced a bias we calculate the average weight per post-synaptic neuron.
If the average weight is below 0.2 we re-initialise the synaptic weights associated to this neuron.
The re-initialisation guarantees that each neuron can learn a unique spatio-temporal receptive fields.
% biological evidence!
In the same line of reasoning, if all weights associated to a post-synaptic are being potentiated, which leads to even stronger synaptic weights due to the correlation-based \ac{stdp} paradigm. In this case we re-initialise the weights if the average weight is above 0.8.
Example receptive fields are depicted in Figure~\ref{fig:example_rf}c.
In biological neural networks it has been observed that weak synapses which are characterised by physically small spine structures get pruned \citep[e.g.\ ][]{Abraham__Robins05,Mitra_etal05,Restivo_etal09} in an activity dependent manner, however, these processes happen typically on much longer time scales such as hours, days or event months.
We took these observation only as an inspiration, but we do not claim to realistically model them.
For more biological realistic models and possible implementation on neuromorphic hardware please refer to \citet{George17}.

\subsection{Sorting}
To better understand what the network is coding for we sort the spike raster plots according to a given weight matrix.
First we compute the pair-wise similarity of the neuron weight vectors, by calculating the Euclidean distance between them.
In the second step we construct a directed graph based on the similarity of each node and allow each node to maximally be connected to any two other nodes.
The resulting graph represents a permutation, in which similar weight vectors are closer to each other compared to orthogonal ones.
In the following we will refer to this by saying the spike raster plot is sorted according to a given synaptic projection or weight matrix.

\section{Results}
\label{sec:results_octa}
We use a simple spatio-temporal pattern to study emergent network dynamics of a single \ac{octa} module.
The stimulus is a rotating bar, with 18 different angular positions and a spatial extent of $10\ x\ 10$ pixels (Figure~\ref{fig:microcircuit}c, input).
The bar itself is encoded as a set of events, similar to what an event-based sensor would perceive if an edge is crossing its pixels.
Additionally we add 20 $\%$ noise events.
The bar rotates with a constant angular velocity of 3300 $\tfrac{\circ}{s}$.
This particular input was chosen because it represents a cyclic, relatively simple spatio-temporal pattern which makes the interpretation of the learned receptive and predictive fields and their temporal sequence more intuitive and easier to understand.
Furthermore, the simplicity of the input makes it possible to more easily asses if a single \ac{octa} module is capable of extracting the prevalent feature of different orientated edges despite the presence of noise, and also to understand which information about the stimulus is encapsulated in which weight matrix.
The spatial dimension was chosen to represent a realistic size receptive fields to later learn feature from actually event-based sensors \citep[for an analysis on the size of receptive fields for event-based sensors please refer to][]{Afshar_etal19}.
The input is connected one-to-one with static synapses.
These connections are only there to relay the events as generated by the input stimulus to $L_4$ neurons.
In the beginning of the simulation all synapses are initialised all-to-all and the respective synaptic weights are randomly sampled from from a $\gamma$-distribution, except those projections marked with a dashed line (see Figure~\ref{fig:microcircuit}c).
The synapses projecting from the pool of excitatory neurons to their respective inhibitory interneurons are randomly initialised with a connectivity probability $p = 0.5$.
Additionally, inhibitory neurons project to themselves with connectivity probability $p = 0.1$.
The excitatory feedforward projections from $L_4$ to $L_{2/3}$ and $L_{5/6}$ respectively have the objective to learn spatio-temporal features, whereas the excitatory recurrent projections should capture the temporal evolution and thus represent a temporal expectation.
The excitatory synapses from $L_{2/3}$ to $L_{5/6}$ on the other hand have the objective to encapsulate the temporal relation between the learned spatio-temporal features.
The respective inhibitory projections should provide the necessary competition among the excitatory neurons and balance activity therein.
A special role is assigned to the projection from $L_{5/6}$ to $L_4$, which should predict inputs by successfully modulating the synaptic efficacy of the post-synaptic partners.
We will start by looking into the activity of $L_{2/3}$ and show that the involved synaptic projections indeed fulfil the aforementioned objectives, before we look into the activity of an entire \ac{octa} module.

\subsection{Spatio-temporal receptive fields}
% All synaptic weights are randomly sampled before we start training (see Figure~\ref{fig:receptive_fields} a).
In total $L_{2/3}$ comprises 49 neurons, which are arranged in a $7x7$ grid (see Figure~\ref{fig:receptive_fields} a).
Each post-synaptic weight vector is rearranged to match the input space to visualise the $10x10$ receptive field (see Figure~\ref{fig:receptive_fields} top row), and the corresponding histogram of the full adjacency matrix between $L_4$ and $L_{2/3}$ is shown below (see Figure~\ref{fig:receptive_fields}, middle row).
To easily assess the information content encapsulated by the receptive fields of neurons in $L_{2/3}$ we calculate the cross-correlation coefficients between all neurons of $L_{2/3}$ and sort the coefficients according to the neuron id (see Figure~\ref{fig:receptive_fields}, bottom row).
Upon initialisation the receptive fields are random which can easily be seen in the sorted cross-correlation coefficients.
Neurons are only highly correlated to themselves, while being uncorrelated to others (see Figure~\ref{fig:receptive_fields} a).
\begin{figure}
    \centerline{\includegraphics[width=17cm, height=8cm]{figures/{receptive_fields_effect_inhibtion}.pdf}}
    \caption[Learned receptive fields.]{Receptive fields of excitatory neurons in $L_{2/3}$. Top row: receptive fields; middle row: histogram of adjacency matrix between $L_{4}$ and $L_{2/3}$; bottom row: sorted cross-correlation coefficients between receptive fields of neurons in $L_{2/3}$. a) Randomly initialised receptive fields before training. Weights are sampled from a $\gamma$-distribution with $k = 0.4$ and $\Sigma = 0.9$ and are clipped between 0 and 1. The average weight is 0.38. The cross-correlation analysis shows that each receptive field is only highly correlated to itself while being un- or anti-correlated to others. b) Receptive fields after 100 revolutions with random-fixed inhibition. Only 28 out of the 49 neurons were able to develop an orientation preference. However, as inhibitory weights are fixed, the learned receptive fields are highly correlated and cover only a subspace of the presented orientations. c) Receptive fields after 100 revolutions with plastic inhibition according to \ref{eq:adp}. 43 out of the 49 neurons were able to develop an orientation preference. The correlation analysis shows that some receptive fields are highly correlated, however, a substantial amount of receptive fields are either uncorrelated or even anti-correlated, nicely spanning the space of presented orientations.}
    \label{fig:receptive_fields}
\end{figure}
As the receptive fields are random in the beginning any presented input will lead to spiking activity in $L_{2/3}$, which in turn activates the inhibitory neurons, which themselves suppress activity in the excitatory pool (see Figure~\ref{fig:rasterplot}a).
The temporal evolution is only governed by the synaptic time constants and does not reflect any temporal aspects of the provided input.
In the first experiment we only adjust the excitatory \ac{stdp} synapses (see Eq.~\ref{eq:stdp}), while keeping the inhibitory synapses fixed ($p = 0.67$).
After 10 $s$ only 28 neurons adjusted their weights to show an orientation preference in their receptive field (see Figure~\ref{fig:receptive_fields}b).
The remaining 21 neurons have random receptive fields, which due to the constant weight decay and despite the reinitialisation have small associated synaptic weights.
The receptive fields are sorted according to the learned recurrent weight matrix.
Neurons with similar receptive fields show a strong similarity in their recurrent weights and thus are placed next to each other.
Apart from the orientation preference, the cross-correlation analysis reveals that the learned receptive fields are highly correlated and only a few receptive fields are in fact anti-correlated (see Figure~\ref{fig:receptive_fields}b, bottom).
Even if a given neuron now tries to tune its weights to reflect a certain orientation, the static inhibitory clamp prevents it from doing so by preventing post-synaptic spiking, which is crucial for \ac{stdp}.
The effect of this static inhibitory clamp not only affects the formation of orientation tuning, but also affects the temporal activity (see Figure~\ref{fig:rasterplot}b).
The spike raster plot is also sorted according to the recurrent weight matrix.
While the feedforward weights from $L_4$ to $L_{2/3}$ incorporate the spatio-temporal structure of the input, the temporal sequence is encoded in the recurrent weights, as the sorted spike raster plot shows that neurons with a particular orientation preference fire repetitively at a particular moment in time (see Figure~\ref{fig:rasterplot}b).
In the following we will refer to this behaviour as temporal alignment.
\begin{figure}[!h]
    \centering
    \includegraphics[width=\textwidth, height=2.5cm]{figures/{effect_of_inhibition}.pdf}
    \caption[Temporal activity before and after training recurrent weights.]{Spike raster plot, i.e. temporal activity, of $L_{2/3}$ before training and without and with plastic inhibition for a full stimulus revolution. Raster plots are sorted according to recurrent weight matrix of $L_{2/3}$. a) Randomly initialised network with randomly sorted neuron indices. No structure in the temporal activity is present. b) Temporal activity after 100 revolutions with random-fixed inhibition. Only 28 out of the 49 neurons were able to develop an orientation preference and thus are represented in the spike raster plot. While the recurrent weight matrix is able to incorporate the temporal structure of the input, the non-plastic inhibition prevents the neurons from being active during the entire stimulus presentation. Furthermore, due to the \ac{stdp} rule, the network's activity starts to synchronise as static inhibition can not prevent it. c) Temporal activity after 100 revolutions with plastic inhibition according to \ref{eq:adp}. 43 out of the 49 neurons were able to develop an orientation preference and thus are represented in the spike raster plot. The plastic inhibition allows the network to utilise more computational resources and thus neurons are active during the entire stimulus presentation. Plastic inhibition also prevents the network from synchronising.}
    \label{fig:rasterplot}
\end{figure}
When examining Figure \ref{fig:rasterplot}b more closely, however, only 28 out of the 49 feature detectors are at all active and as the correlation analysis of the receptive fields suggest, they tend to temporally cluster due to similar receptive fields, which ultimately will lead to synchronised activity in $L_{2/3}$.
The synchronisation is a natural consequence of correlation-based learning rules, such as \ac{stdp}.
Up to now we kept the inhibition provided by the inhibitory interneurons static.
This clearly provides the necessary competition among the excitatory feature detector neurons to learn different spatio-temporal features.
However, it has the drawback that over time the receptive fields and the temporal network activity tend to synchronise, and thus do not encode any unique/useful information about the stimulus as the activity is only determined by the synaptic constants between the excitatory and inhibitory neurons and vice versa, but not by any learned weights.
% In the second experiment we make the inhibitory synapses to the proposed \ac{adp} rule, thus inhibitory synapses can update their weights according to equation ~\ref{eq:adp}.
To overcome this limitation and allow the network to use more computational resources, i.e neuron and synapses, we initialise the inhibitory to excitatory projection as all-to-all and make the weights subject to \ac{adp}.
The inhibition, which is needed to learn different orientation tuning in the excitatory neuron pool, is adapted according to Equation~\ref{eq:adp}, to allow each neuron to be equally active which results in more diverse receptive fields (compare Figure~\ref{fig:receptive_fields}b \& c).
This increased ability to differentiate between more orientations is also reflected in the corresponding correlation analysis (see Figure~\ref{fig:receptive_fields}c, bottom).
Inhibitory weights which are too weak in the beginning lead to an over-excitation, and thus to similar orientation tuning of multiple neurons in $L_{2/3}$.
This increased activity level is reflected in the normalised post-synaptic variable $\hat{\sigma}_i$ and hence these weights are strengthened according to Equation~\ref{eq:adp} (see Figure~\ref{fig:adp}).
On the other hand, inhibitory projections which are too strong and prevent any activity in their respective excitatory partners in $L_{2/3}$ are weakened, to allow the neuron to tune its weights to a particular orientation.
By adjusting the inhibitory weights according to Eq.~\ref{eq:adp}, 43 neurons out of the 49 learn a preferred orientation.
\begin{figure}[!t]
    \centering
    \includegraphics[width=\textwidth, height=5cm]{figures/{activity_proxy_single_trace}.png}
    \caption[Effect of \acl{adp} mechanism to adjust inhibitory weights.]{Effect of \acf{adp} on normalised post-synaptic activity $\hat{\sigma}_i$ of neurons in $L_{2/3}$. In total 588 traces are shown. A single normalised post-synaptic activity trace is plotted in black for better visibility. The proposed inhibitory plasticity rule is capable of adjusting the inhibitory weights, such that the difference between $\hat{\sigma}_i$ and $\sigma_{target}$ is minimised. This leads not only to more neurons being recruited to tune their weights to a particular orientation, but also balances the overall activity, such that it helps to prevent oscillations from occurring.}
    \label{fig:adp}
\end{figure}
Hence, more computational resources can be leveraged to learn and encode the presented spatio-temporal pattern.
The amount of allowed activity is set by $\sigma_{target}$, which can take any value between 0 and 1 and is uniformly initialised between  $0.3$ and $0.5$ for each synapse.
When setting $\sigma_{target}$ equal to 1, meaning no inhibitory competition is provided, the learned receptive fields are the same for each neuron, a superposition of all presented orientations .
On the other hand, a $\sigma_{target}$ of 0 leads to maximal inhibition, which causes the network to oscillate and learn a single preferred orientation for all neurons in $L_{2/3}$.
Despite the inhibitory weight adaptation introduced by \ac{adp}, the recurrent excitatory weights capture the temporal evolution of the stimulus and the network activity is temporally aligned (see Figure~\ref{fig:rasterplot}c).
The inhibitory plasticity successfully balances the network activity by minimizing the difference between the post-synaptic activity $\hat{\sigma}_i$ and the respective activity target $\sigma_{target}$ (see Figure~\ref{fig:adp}).
Additionally, the proposed plasticity mechanism prevents the network from synchronising, as the learned receptive field are more diverse and each neuron has a unique temporal activity during the presentation of the stimulus.
\subsection{Mismatch and synaptic temporal prediction}
Oscillations represent a severe limitation for \acp{snn} if the oscillatory frequency is solely determined by the synaptic time constants.
The network's activity starts to oscillate either if inhibition is too strong and not subject to plasticity or if the network is exposed longer than 15 seconds to the stimulus.
The latter reason is due to fact that the excitatory synapses are altered according to \ac{stdp}.
\ac{stdp} is a temporal correlation based learning rule, which has the consequence that synaptic weights get strengthened if the respective pre and post synaptic neurons show temporally correlated activity.
Ultimately, this will cause already correlated activity to become more correlated up to the point where all weights are high.
This has the consequence that excitatory neurons will emit spikes at the same point in time which in turn will trigger an avalanche of inhibitory spikes, preventing any neuron from firing until excitatory drive is again strong enough.
Hence, the network will exhibit oscillations and lose all previously encoded information as the dominant source of activity is internally generated and determined by the excitatory-inhibitory synaptic time constants.\\
As previously discussed, neuromorphic analogue sensory-processing systems are subject to device mismatch.
The proposed meta-architecture was designed to provide an unsupervised, local feature extraction mechanism for neuromorphic sensory-processing systems.
To overcome the aforementioned limitation and test if an \ac{octa} module would be suitable for hardware emulation we added mismatch to our simulation.
It turns out that the heterogeneity does not need to be compensated by population averaging as previously proposed (see Chapter~\ref{ch:reactivecontrol}), but in fact in combination with \ac{stdp} and \ac{adp} provides the network with sufficient noise to prevent synchronous oscillations from dominating the network's activity.
\begin{figure}
    \centerline{\includegraphics[width=17cm, height=11cm]{figures/{rasterplot_mismatch_v2}.pdf}}
    \caption[Network activity and effect of mismatch.]{Activity of a single \acf{octa} module. The blue box represents the temporal activity of $L_4$, the green box represents temporal activity of $L_{2/3}$ after sorting according to its recurrent weight matrix. The orange box represents the temporal activity of $L_{5/6}$ after sorting according to weight matrix between $L_{2/3}$ and $L_{5/6}$. All parameters of the network are subject to mismatch, i.e. show $\approx 20 \%$ variability.
    Not only is the network capable, despite the additional noise, of capturing the prevalent spatio-temporal statistics of the input, but it learns phase shifted predictive fields to temporally predict the input. Furthermore, adding mismatch allows the involved plasticity mechanisms to find a stable operating regime, without starting to oscillate and proves that the proposed meta-architecture is suited for analogue neuromorphic processing systems.}
    \label{fig:mismatch}
\end{figure}
As in the previous simulation the network starts from randomly initialised weights and all plastic parameters are trained jointly.
Both excitatory feedforward projections from $L_4$ to $L_{2/3}$ and $L_{5/6}$ respectively learn the statistically prevalent orientations of the input, which is reflected in the different receptive fields (see Figure~\ref{fig:mismatch}).
Note that, in contrast to previous experiments, the network is stimulated for 30 seconds (also 40, 50, 80 and 150 seconds were tested, showing similar network activity and receptive field formation).
Despite the longer presentation of the input pattern the network does not start to oscillate.
The combination of excitatory-inhibitory balance introduced by \ac{adp} and inhomogeneous network's parameters introduced by simulating device mismatch successfully prevented the network from oscillating.\\
The temporal activity of $L_{2/3}$, which is sorted according to the recurrent weight matrix shows the same temporal alignment as in previous experiments (see Figure~\ref{fig:mismatch}, $L_{2/3}$ spike raster plot).
The temporal activity of $L_{5/6}$ shows a similar temporal alignment, however, the spike raster plot is sorted according to the weight matrix between $L_{2/3}$ and $L_{5/6}$.
This shows that the temporal information, i.e the input sequence, is preserved in the synaptic weight matrix between $L_{2/3}$ and $L_{5/6}$ (see Figure~\ref{fig:mismatch}, $L_{5/6}$ spike raster plot).
The only aforementioned synaptic objective we have not yet addressed is the one of the synaptic projections from $L_{5/6}$ back to its input population $L_4$.
We hoped they would temporally predict the input.
In contrast to all other synaptic projections, the predictive synapses are not learning either an excitatory or inhibitory weight, but a synaptic gain value $\phi$, which upon pre-synaptic spiking lowers the post-synaptic gain of the feedforward synapses of $L_4$ (see Eq.~\ref{eq:ase_dyn} \&~\ref{eq:stgm}).
The synaptic gain is initialised in the beginning to 1 and the the respective predictive gain value $\phi$ is initialised to 0, thus any early activity in $L_{5/6}$ has no or very little effect on the synaptic efficacy.
The incoming receptive fields to $L_{5/6}$, as well as the outgoing predictive fields are sorted according to the weight matrix from $L_{2/3}$ to $L_{5/6}$.
While the incoming receptive fields are driven by input and respond to similar orientations as their $L_{2/3}$counterparts, the outgoing predictive fields of the same neuron tend to have a phase shifted orientation preference.
The phase shift is approximately 100$^\circ$ (compare the receptive and predictive fields of $L_{5/6}$ in Figure~\ref{fig:mismatch}).
We will discuss the underlying cause of the learned phase shift between receptive and predictive fields later.
% This phase shift can not be explained by the synaptic transmission delay between $L_4$ and $L_{5/6}$, but emerges from the internal network dynamics.



% systems delay is present
% but when modulating the rotational velocity the incorporated phaseshift changes as well


\section{Discussion}
\label{sec:octa_discussion}
We presented an \acf{octa} meta-architecture for recurrent \acfp{snn}, which represents a modular building block for unsupervised feature extraction suited for asynchronous event-based sensory-processing systems.
The recurrent structure is inspired by the recurrent organisation principles found in neocortical columns \citep[][\& see Figure~\ref{fig:microcircuit}]{Douglas_etal89,Binzegger_etal04}.
We showed that a single \ac{octa} module is capable of extracting statistically prevalent spatio-temporal features from a stream of asynchronous events, despite noise present at its sensory inputs and in its computing elements, e.g.\ due to device mismatch.
We demonstrated that device mismatch is a feature which in combination with an inhibitory plasticity mechanism leads to stable receptive fields and balanced network activity.
We further showed that the recurrent network structure enables the network to learn temporal predictions which are encoded in the predictive fields, which are tuned to the input, hence, the network has an implicit temporal representation of time in ongoing processing (see Figure~\ref{fig:mismatch}).\\
% Additionally we demonstrated that am \ac{octa} module can be emulated \\
The feedforward synapses from $L_{4}$ to $L_{2/3}$ and $L_{5/6}$ respectively incorporate spatio-temporally correlated features which result in orientation tuning, whereas the recurrent synaptic projections incorporate the temporal sequence of the stimulus.
The inhibitory projections into their excitatory counterpart provide sufficient competition, while balancing the overall excitatory activity, allowing the excitatory neurons to capture the spatio-temporal evolution of the input (see Figure~\ref{fig:receptive_fields} \&~\ref{fig:rasterplot}).

\paragraph{Recurrent weight matrix:}
To understand the computational role of the learned recurrent excitatory projections we calculated the Euclidean distance, i.e.\ similarity, of each pair of post-synaptic weight vector.
We used the similarity between each weight vector to construct a directed graph in which similar vectors or nodes are placed next to each other and with each node maximally connected to two other nodes.
As each node has an index, the directed graph provides us with a permutation of indices compared to the original neuron indices.
The permutation was used to sort the temporal activity covered in the spike raster plot of the neurons of $L_{2/3}$ (see Figure~\ref{fig:rasterplot}).
This sorting procedure revealed that the temporal sequence of the input is indeed incorporated in the recurrent weight matrix, as the sorted spike raster plot showed that particular neurons fire at particular times and their corresponding receptive fields are tuned to similar orientations (see Figure~\ref{fig:receptive_fields} \&~\ref{fig:rasterplot}).
Due to the temporal sequence of the presented input the observed temporal alignment can be understood as expressing a temporal expectation, since neurons which show similar receptive fields are firing right after each other.\\
Kappel and colleagues reported previously that when excitatory recurrent weights of a soft \ac{wta} network are subject to \ac{stdp}, these recurrent connections represent expectations equal to the expectation step in hidden markov process \citep{Kappel_etal14}, meaning that these connections are capable of learning a temporal sequence.
While \citet{Kappel_etal14} showed using theoretical analysis and using computer simulations that this represents a useful feature for a variety of cognitive tasks, we could demonstrate that their findings are also valid in the context of event-based inputs and noisy sensing and computing.\\
The uni-directional projection between neurons in $L_{2/3}$ and $L_{5/6}$, on the other hand, preserves the learned temporal structure of the excitatory recurrent weights of $L_{2/3}$ and $L_{5/6}$.
The temporal alignment observed in $L_{2/3}$ is present in $L_{5/6}$, even though we used the similarity in the uni-directional projection to sort the temporal activity of $L_{5/6}$ (see Figure~\ref{fig:mismatch}).

% + stochastic EM Nessler_etal13
%!
\begin{figure}[!t]
    \centerline{\includegraphics[width=15cm, height=8cm]{figures/{examples_additional_observation}.pdf}}
    \caption[Example receptive fields and effect of non-balanced network activity.]{Example receptive fields and effect of non-balanced network activity. a) Receptive field as learned by neurons in $L_{2/3}$ with $\sigma_{target}$ set to zero (top) and set to one (bottom). A value of zero represents maximally strong inhibition is provided. This leads to the formation of a single receptive field which is shared among all neurons in $L_{2/3}$. A value of one indicates that all inhibitory synaptic weights will converge to zero and no inhibition is provided. The absence of inhibition leads to the formation of receptive field which is characterised by the superposition of all prevalent orientations of the presented rotating bar. b) In the case of $\sigma_{target}$ being one only a single orientation is represented in the receptive fields which is also reflected in the spike raster plot. The activity is synchronised. Due to the too strong inhibition the network starts to oscillate, which also explains the formation of a single receptive field. c) Example receptive fields which trigger the re-initialisation of associated post-synaptic weight vector.}
    \label{fig:example_rf}
\end{figure}
\paragraph{Stability and the effect of \acl{adp}:}
The excitatory feedforward and lateral recurrent projections enable the network to learn the spatio-temporal statistics of the presented input.
The inhibitory projections are crucial in this context, as these synapses act as an activity regulariser, while introducing competition between excitatory neurons.
% and thus enable the network to use more computational resources to capture the spatio-temporal statistics of the input .
If we would turn inhibition off, by setting $\sigma_{target}$ to 1, the learned receptive fields would represent a superposition of the input, thus being highly correlated (see Figure~\ref{fig:example_rf}a, bottom).
Furthermore, the network's activity would become temporally correlated and constant throughout the stimulus presentation.
This is clearly due to \ac{stdp}, as it is a correlation-based learning paradigm \citep{Markram_etal97,Bi_Poo98,Song_etal00}.
If no counter mechanism is present to decorrelate activity, \ac{stdp} enforces already correlated activity even more.
If we were, on the other hand, to set $\sigma_{target}$ to 0, meaning that we install maximally strong inhibition similar to a connectivity probability of 1 between inhibitory and excitatory neurons, we would observe two effects.
First, the network would start to oscillate, as excitatory activity would lead to a massive injection of inhibitory currents back into the excitatory pool, which in turn would lead to strong inhibitory activity (see Figure~\ref{fig:example_rf}b).
The frequency of oscillation would be independent of the input and only determined by the synaptic time constants between the excitatory and inhibitory neurons \citep{vanVreeswijk_etal94}.
Secondly, the oscillations thus introduced would lead to a single learned receptive field which is the same for all excitatory neurons, as the post-synaptic spike time, which is used by \ac{stdp}, would be same for all excitatory neurons (see Figure~\ref{fig:example_rf}a, top).\\
If we initialise the inhibitory weights with connectivity probability lower than 1, e.g.\ 0.7, and do not adjust the associated synaptic weight, the network would thereafter be capable of encapsulating the spatio-temporal statistics of the input and learning receptive fields for different orientation tunings (see Figure~\ref{fig:receptive_fields}b \&~\ref{fig:rasterplot}b).
The static inhibition would lead, however, to oscillatory behaviour, as the excitatory weights increase due to \ac{stdp} and introduce correlated activity which can not be compensated by the static inhibitory clamp (see Figure~\ref{fig:rasterplot}b).
Furthermore, the static inhibitory clamp prevents the network from utilizing all its available computational units, e.g.\ neurons, because the associated inhibitory weight for a subset of excitatory neurons is too high (compare Figure~\ref{fig:rasterplot}b \& c).
If we use Equation \ref{eq:adp} with intermediate, randomly distributed values of $\sigma_{target}$ to regulate the strength of inhibitory synapses, we can effectively balance excitatory activity (see Figure~\ref{fig:ase_octa}), while allowing the network to use more neurons to tune their weights to learn a sensible receptive fields and thus cover all presented orientations (compare Figure~\ref{fig:rasterplot}b \& c and note the temporal overlap spikes of neuron 6 and  49).
When combining \ac{adp} with heterogeneous network parameters, i.e. device mismatch, we effectively use more available computational resources, i.e.\ neurons, and prevent the network from starting to oscillate which would lead to a loss of information being encoded in the network (compare Figure~\ref{fig:example_rf}b \&~\ref{fig:mismatch}).\\
If we would like to operate the network in rate-mode, we could rely on reverse \ac{stdp} \citep{Foldiak90,Roberts_Leen10} to adjust the inhibitory weights.
The inhibitory weight can only successfully be increased if the excitatory activity continuously proceeds the inhibitory activity, since their is no direct causal link between inhibitory and excitatory activity, i.e.\ inhibition does not trigger post-synaptic activity but it rather prevents it.
Neural activity, however, cannot be assumed to be constant over longer periods of time in the context of asynchronous sensing and processing.\\
Vogels and colleagues proposed an alternative learning paradigm to adjust inhibitory weights to balance ongoing processing in excitatory neurons \citep{Vogels_etal11}.
This inhibitory plasticity rule adjusts the inhibitory synaptic strength based on a pre-defined target rate.
Their plasticity rule is similar to the \ac{adp} rule proposed here, however, their rate-based rule only uses post-synaptic super-threshold activity, i.e.\ spikes, to adjust the synaptic weights.
The rule from Vogels does not consider sub-threshold membrane fluctuations, which are more likely to be present and informative in asynchronous inputs.\\
Both \citet{Vogels_etal11} and the learning rule proposed here adjusts synaptic weights to balance activity.
\citet{Triesch05} proposed a mechanism, called intrinsic plasticity, which adjusts intrinsic parameters of the neuron model to ensure sparse activity, such that the overall activity stays bounded.
However, the parameters which are adjusted change the transfer function of the neuron which transmits information solely in the output firing rate, but not in the precise timing of spikes.
The adaptive thresholding approach of \citet{Afshar_etal19} seems more appealing in the context of event-based data to achieve the homoeostatic effects of intrinsic plasticity, as it was already demonstrated to work in the context of temporal-coded, time-continuous inputs.\\

\paragraph{\acl{stdp}:}
The implemented \acf{stdp} rule was original proposed by \citet{Song_etal00}.
This plasticity rule captures weight update dynamics depending on pre- and post-synaptic spike times, i.e.\ pairs of spikes, found in biological neural networks \citep{Bi_Poo98,Markram_etal97}.
Equation~\ref{eq:stdp}, however, only accounts for one specific temporal relation of pre- and post-synaptic spike pairs.
In the last two decades more temporal relations between pre- and post-synaptic spiking to update the synaptic weight have been experimentally measured \citep[e.g.\ ][]{Graupner_Brunel12,Feldman12,Roberts_Bell02,Abbott_Nelson00}.
An interesting learning rule to account for more experimentally observed temporal weight update dynamics relies on triplets of spike, i.e.\ post-pre-post instead of classical pairs of spikes \citep{Pfister_Gerstner06,Gjorgjieva_etal11}.
The incorporation of triplet \ac{stdp} represents an interesting addition to the system proposed here and will be subject to future research.
Especially, it would be interesting to compare the effect of classical and triplet \ac{stdp} on the stability of the network and on the incorporation of temporal sequences of learned spatio-temporal features.
% \begin{itemize}
    % \item We have presented a modular building block for \acp{snn} to learn spatio-temporal patterns
    % \item ff weights encode features, rec. exc. connections learn temporal sequences similar to \citep{Kappel_etal14}
    % \item inhibitory synapses act as activity regularizer and if subject to learning enable the network to use more comp. resources
    % \item often inh. is kept static
    % \item talk about inh. clamp
    % \item or if a given network is encoding its information in rate-based reverse stdp protocols are used
    % \item plastic inh. allows to uilise more comp. resources and in combination with inhomogeneous network hyperparmaters prevent oscillations
    % \item No inhibition
    % \item static inhibition (p=1.0) oscilaations (Vreeswijk1994)
    % \item static random inhibition
    % \item Activity Dependent Plasticity + biological evidence
% \end{itemize}
\subsection{Comparison to state-of-the-art event-based feature extraction}
% The proposed \ac{octa} network is capable of learning spatio-temporal statistics of event-based inputs with an implicit temporal representation of time in ongoing processing.
% It represents an unsupervised, modular building block for \acp{snn} which is suited to neuromorphic processing systems.
% The \acf{ase} mechanism, which was originally proposed to extract temporal correlations from spatially neighbouring events to estimate optic flow from event-based sensors \citep[][\& see Chapter~\ref{ch:semd}]{Milde_etal18}, was shown to encode errors locally by temporal predictions which adaptively scales synaptic efficacy.\\
% \citet{Bichler_etal12} were able to show that by combining \ac{stdp} in feedforward synaptic projections in hard \ac{wta} to be capable of learning spatio-temporal features for tracking moving objects.
% Additionally, the implemented \ac{stdp} rule had an infinitely long depression window and the receptive fields covered the entire retinal input space.
% \citet{Lagorce_etal15} could demonstrate by converting the asynchronous stream of events into analogue traces, what was later called a time surface (see Section~\ref{sec:ebalgorithms}), and presenting these analogue traces to multiple \acfp{esn} \citep{Jaeger02}, their network was capable of extracting spatio-temporal features for classification \citep{Lagorce_etal15}.
% Lagorce showed that this \ac{esn}-based approach is capable of incorporating multiple receptive fields with unique spatio-temporal features, which was achieved by incorporating a \ac{wta} mechanism.
% Each \ac{esn} is capable of representing a complex spatio-temporal pattern, but lateral connections served only to introduce competition among feature detectors \citep{Lagorce_etal15}.

% The incorporation of \acp{esn} in feature extraction allowed the network to represent the temporal information present in the input, but
% After this feature detection the network conveyed information using integrate and fire neurons, which transmit information in their firing rate \citep{Lagorce_etal15}.
Events as emitted by event-based sensors can be converted into so-called time surfaces (see Section~\ref{sec:ebalgorithms}).
These time surfaces can either be classified using \acfp{esn} \citep{Lagorce_etal15} or can be classified by template matching \citep{Lagorce_etal17, Afshar_etal19}.
The feature templates can be stacked together to form a hierarchical feature extraction system \citep{Lagorce_etal17, Afshar_etal19}.
The presence of a given time surface can either be conveyed (i) using integrate and fire neurons, which transmit information in their firing rate \citep{Lagorce_etal15} or can be conveyed (ii) in the form of an asynchronous \ac{aer} events in which the payload encodes the presence of a given time surface \citep{Lagorce_etal17, Afshar_etal19}.
Despite extracting spatio-temporal patterns from precisely timed events the output produced in the former case encodes information solely in rate-mode, meaning that for any downstream neuron the spike train does not carry additional information in its temporal domain.
In the latter case the time when a time surface event is generated carries information which can used by downstream processing stage to classify moving \citep{Afshar_etal19} and stationary objects \citep{Lagorce_etal17}, and even to perform face classification \citep{Lagorce_etal17}.\\
% In a later study \citet{Lagorce_etal17} further demonstrated that when converting the asynchronous stream of events into a time surface, it is sufficient to match time surface templates to perform classification \citep{Lagorce_etal17}.
\citet{Afshar_etal19} extended the template matching approach of \citet{Lagorce_etal17} by introducing adaptive thresholds, which provided faster convergence and more stable receptive fields.
% Both approaches can be efficiently implemented in conventional clock-based computing architectures and the timing of detected time surface templates is meaningful.
However, recurrent interactions between feature detectors serve only to introduce competition among the feature detectors \citep{Lagorce_etal15,Lagorce_etal17,Afshar_etal19} and need to happen in some cases instantaneously \citep{Lagorce_etal17,Afshar_etal19}.
% The lateral connections in their \ac{snn} were not subject to learning and purely served a competitive purpose among the excitatory neurons.
% conveyed information using integrate and fire neurons, which transmit information in their firing rate \citep{Lagorce_etal15}
The instantaneous inhibition makes an implementation on spiking neuromorphic processors complicated and is biologically rather implausible.\\
We could clearly demonstrate that excitatory recurrent synaptic projections when subject to learning carry information about the temporal sequence of presented stimuli and that inhibitory recurrent projections can balance excitatory activity and thus drive the network into an efficient operation regime.
The adaptive threshold approach of \citet{Afshar_etal19} helped the network to use its computational resources and allocate them equally across available spatio-temporal features, similar to the proposed inhibitory plasticity mechanism.
A combination of the adaptive threshold with plastic inhibition represents an interesting case study for future investigations.\\
The present work shows for the first time that ongoing processing can encapsulate an implicit representation of time, which emerges from the recurrent organisation of recurrently connected excitatory-inhibitory networks inspired by neocortical organisation principles.
This temporal representation is preserved throughout processing and is thus also available for the next processing stage.
% \subsection{Stability}
% \begin{itemize}
%     \item stdp as correlation based learning rule enables in the beginning to capture spatio-temporally correlated patterns
%     \item when input is continuously presented the correlative nature, however, strengthen already strong synapses, thus synchronising the network's activity. This has the consequence, as the core of the proposed architecture are EI networks that neurons start to oscillate and typically show homogeneous receptive fields and spike timing becomes irrelevant
% \end{itemize}
\subsection{Limitations}
The present study simulated only a single \ac{octa} module and investigated the computational role of the feedforward, lateral recurrent and feedback projections within the \ac{octa} module.
When the \acf{isi} of events presented inputs increases beyond the specified temporal window of the \ac{stdp} mechanism, the feedforward and lateral recurrent connections, which encapsulate spatio-temporal features and their sequence, cannot be properly adjusted to incorporate the input statistics.
This is simply because no pair of spikes would fall into the temporal window of \ac{stdp} and hence no potentiation or depression of any weight would take place.
This scenario, however, is usually not the case for an event-based sensor, due to its time-continuous mode of operation and its high temporal resolution.
Only if neither the environment nor the sensor moves, the \ac{isi} of events at the input would fall outside the the temporal window of \ac{stdp} in which case no learning should take place.
% \begin{itemize}
%     \item So far we only simulated a single \ac{octa} module
%     \item if the inputs is non-continuous or ISI is longer than the \ac{stdp} time window feature can not be picked up
%     \item this, however, is usually not the case for a continuously moving sensor

% \end{itemize}
\subsubsection{Spike-time dependent gain plasticity}
We reported earlier that a single \ac{octa} module learns phase shifted predictive fields with respect to the receptive fields.
To understand the underlying cause for the observed phase shift between corresponding receptive and predictive fields of neurons in $L_{5/6}$, we systemically vary the angular velocity $\omega$ of the input and calculate the average phase shift $\Delta \Phi$ between corresponding preferred orientations of receptive and predictive fields (see Figure~\ref{fig:phaseshift}).
The phase shift is learned via a plasticity rule which similar to classical \ac{stdp} uses temporal correlation of pre and post-synaptic spike timing, however, instead of learning a synaptic weight, the proposed plasticity modulates the synaptic gain (see Eq.~\ref{eq:stgm}).
The learned synaptic gain modulates the synaptic efficacy according to~\ref{eq:ase_dyn}.
Hence, the information transmitted from $L_{4}$ to $L_{2/3}$ and $L_{5/6}$ respectively is gated, if the learned predictive field from $L_{5/6}$ temporally matches the input.
\begin{figure}
    \centering
    \includegraphics[width=\textwidth, height=7cm]{figures/{phaseshift}.pdf}
    \caption[Learned phase shift for different rotational speeds.]{Phase shift between receptive and predictive fields of $L_{5/6}$ for different rotational velocities of the presented bar.}
    \label{fig:phaseshift}
\end{figure}
When the rotational speed of the presented rotating bar is increased the represented phase shift between the learned, preferred orientations of corresponding receptive and predictive fields decreases.
This phase shift is partially caused by the accumulated synaptic and neuronal delays of the system, thus the system has a preferred speed.
This partially explains the constant phase shift angular velocities close to its preferred angular velocity in Figure~\ref{fig:phaseshift}.
However, if the observed phase shift would be an effect of the accumulated system's delays only, the phase shift should be constant irrespective of the angular velocity.
If synaptic weights become too weak we re-initialise the synaptic weights (see Section~\ref{sec:remarks_learning}).
However, during this re-initialisation we keep the associated synaptic time constant fixed, meaning that at the moment the system can not learn optimal time constants for a given presented angular velocity (see Section~\ref{sec:octapus} for possible mechanisms to learn synaptic time constants).\\
The synaptic gain modulation is based on precise spike timing, yet neuromorphic systems have certain limitations (see Sections~\ref{sec:ebsensors} \&~\ref{sec:ebprocessors} and Chapter~\ref{ch:reactivecontrol}).
% Neuromorphic sensory-processing systems are subject to device mismatch.
% Furthermore, the response of neuromorphic sensors, such as the \ac{dvs}, is noisy.
% This noise is,due to shot noise, which triggers events without a cause \citep{Lichtsteiner_etal08,Yang_etal15}.
Hence, an algorithm emulated on neuromorphic hardware might be subject to temporal jitter, which in the context of the temporal predictions based on precise spike timing might be fatal.
However, temporal jitter, i.e.\ uncertainty, may actually be beneficial, since no prediction, top-down attention, or bottom-up prediction, will be perfect in time and thus will never set the efficacy to the highest possible value (an avalanche of spikes), nor set the efficacy to the lowest possible value (killing the activity of the network completely).

\subsection{Remarks for hardware emulation}
\label{sec:octa_hw}
\begin{figure}[!t]
    \centerline{\includegraphics[width=14cm, height=4cm]{figures/{receptive_fields_mismatch_quantized}.pdf}}
    \caption[Quantised receptive fields.]{Receptive fields of $L_{2/3}$. a) Learned receptive fields with 32-bit floating point precision. b) Quantised receptive field with 1-bit precision. Weights are quantised to meet limited weight resolution on the \ac{dynap}. Each neuron only has 64 possible synaptic inputs. However, $L_{2/3}$ has in total 3 different weight matrices: feedforward inputs from $L_{4}$, recurrent inputs from $L_{2/3}$ and inhibitory inputs from the interneuron population. We quantise the weight matrix in such a way that only the 20 strongest weights are kept, whereas all others are pruned. c) Raster plot activity of $L_{2/3}$ emulated on the \ac{dynap}. Raster plot is sorted according to the learned recurrent weights within $L_{2/3}$. Despite the one-step quantisation and a different mismatch distribution present on the \ac{dynap} compared to the simulated mismatch the network shows the same temporal alignment as previously observed in software simulation. Note that the input is slowed down by a factor of 3 which has the effect that 1 revolution lasts 300 ms instead of 100 ms.}
    \label{fig:quantised_weights}
\end{figure}
A major motivation of the present work was to construct a neurally implemented algorithm to learn spatio-temporal features as emitted by a \ac{dvs}, suited for neuromorphic processors.
Analogue-digital sub-threshold neuromorphic systems, such as the \ac{dynap} \citep{Moradi_etal18},  have severe constraints on the bit-precision of synaptic weights (see Section~\ref{sec:ebprocessors}).
The simulations conducted in this study used, as is standard in computer simulations, a 32-bit floating point representation of synaptic weights.
Secondly, adjacency matrices which were subject to plasticity were initialised with all-to-all connectivity.
From a simulation point of view the all-to-all initialisation can easily be adjusted, when incorporating structural plasticity \citep[e.g.\ ][]{George_etal15,Spiess_etal16}.
However, the mismatch between simulated weight resolution and the effective weight resolution of some neuromorphic processor is more severe (see Section~\ref{sec:ebprocessors}).\\
\paragraph{Weight quantisation:} To transfer parameters from a higher bit resolution to hardware which has a limited bit resolution compression scheme, such as quantisation, are used during training \citep{Stromatias_etal15a, Courbariaux_etal15,Gysel18,Milde_etal17a}.
A key feature of these quantisation schemes is that the available number of bits can be distributed dynamically for each parameter.
In the context of neuromorphic processors, such as the \ac{dynap}, we cannot afford to dynamically distribute the available number of bits due to weight sharing.
The weight which is set by a bias current for each core on the \ac{dynap} is shared among the 256 neurons present in this core \citep{Moradi_etal18}.
Furthermore, each synaptic input can only take 1 out of 4 absolute levels, i.e.\ $I_{WEIGHT_{EXC_{F}}}$, $I_{WEIGHT_{EXC_{S}}}$, $I_{WEIGHT_{INH_{F}}}$ or $I_{WEIGHT_{INH_{S}}}$ where $I$ represents a bias current for the distinct excitatory (EXC), inhibitory (INH), fast (F) and slow (S) \ac{dpi} synapse circuits (see Section~\ref{sec:semd_hw} for schematics of the\ac{dpi} synapse circuit and see Figure 9 in \citep{Moradi_etal18}).
\begin{figure}[!t]
    \centering
    \includegraphics[width=8cm, height=4.5cm]{figures/{oscilloscope_traces}.pdf}
    \caption[Membrane potential of hardware neurons.]{Example membrane potential traces of recurrently connected single excitatory and inhibitory neurons of $L_{2/3}$ emulated on the \ac{dynap}. The learned inhibitory projections, whose weights were also quantised, are capable of balancing the excitatory activity. The recurrent excitatory inputs prime the neural activity (note excitatory ripples) and in conjunction with the feedforward activity elicit on average 1 to 3 spikes per neuron per preferred orientation. Note that the input is slowed down by a factor of 3 which has the effect that 1 revolution lasts 300 ms instead of 100 ms.}
    \label{fig:vmem_traces}
\end{figure}
Furthermore, each neuron on the \ac{dynap} can only integrate 64 distinct synaptic inputs \citep{Moradi_etal18}.
Each synaptic projection within an \ac{octa} module carries sensible information about the spatio-temporal statistics of the presented input.
The total number of non-zero weights learned in the software simulation exceeds by far the 64 available synaptic inputs.
Hence, we cannot use the the trick of emulating a higher weight resolution by varying the connectivity probability between two populations \citep[][\& see \ref{ch:reactivecontrol}]{Milde_etal17b,Milde_etal17,Blum_etal17}.\\

\paragraph{Mapping learned weights on hardware:} A single \ac{octa} module has 3 synaptic projections (feedforward, recurrent excitatory and recurrent inhibitory) within $L_{2/3}$ and 4 synaptic projections (feedforward, recurrent excitatory, recurrent inhibitory and excitatory feedback) within $L_{5/6}$.
To perform a first proof of concept of the applicability of parts of \ac{octa}, i.e.\ $L_{2/3}$, on the \ac{dynap} we follow a simple heuristic: Each synaptic projection can occupy 20 out of the 64 available inputs.
We select 20 strongest weights by quantising the weights such that all other weights are set to zero (compare Figure~\ref{fig:quantised_weights}a \& b).
We use the fast excitatory \ac{dpi} circuit for the feedforward input synapses and the slow excitatory \ac{dpi} circuit for the excitatory recurrent synapses.
The inhibitory synapses use the fast inhibitory \ac{dpi} circuit.

\paragraph{Emulating \ac{octa} on hardware:} After quantising all weight matrices projecting into $L_{2/3}$ we stimulate the \ac{dynap} with the same stimulus and record the network's activity (see Figure~\ref{fig:quantised_weights}c).
The network activity shows the same temporal alignment along the different presented orientations.
Note that we do not need to use a population average to cope with device mismatch.
Furthermore, the inhibitory projections balance the excitatory activity, which is driven by the conjunction of recurrent and feedforward excitatory inputs (see Figure~\ref{fig:vmem_traces}).
This experiment confirms that simulating mismatch and optimizing the weights leads to not only stable network activity, but also allows us to transfer learned synaptic weights to otherwise non-plastic neuromorphic processors.
This finding will allow us in the future to explore different kinds of learning rules, before we can build a system which incorporates the necessary learning mechanism directly in hardware.
%
% MENTION THAT WE DO NOT NEED TO COMPENSATE MISMATCH USING A SPOPULATION CODE
%
% \begin{itemize}
    % \item Simulations were done using 32 bit floating point representation of the weights
    % \item Secondly, we start from an all-to-all connectivity in local RF, which still represent 100 synaptic inputs per neuron at the beginning
    % \item neuromorphic HW such as the \ac{dynap} \citep{Moradi_etal18} only feature 64 inputs per neuron. To port a trained network we can quantize synatic weights. However, normal weight quantization \cite{Stromatias_etal15a, Courbariaux_Bengio15,Gysel18,Milde_etal17a} can not be used to optimise the weights.
    % \item because of 2 absolute possible weights per 256 neurons.
    % \item show 20 strongest weights as potential quantization scheme
    % \item quantize receptive field plot
    % \item raster plot of $L_{2/3}$ emulated on hardware
% \end{itemize}
\subsection{Conclusion}
% We resented a neurally implemented building block for unsupervised event-based feature extraction.
% The building block is suited for neuromorphic hardware as it can deal with mismatch at both the sensory and the processing level.
% We were able to show that device mismatch present on analogue neuromorphic hardware is a useful feature, if we incorporate synaptic plasticity in excitatory and inhibitory synapses to dynamically adjust the network's parameters.
We showed and argued how the proposed meta-architecture opens novel perspectives and understanding of how temporal information present in an event-based computing paradigm can be implicitly represented in ongoing processing.

% \begin{itemize}
%     \item presented a neurally implemented building block for unsupervised event-based feature extraction
%     \item the building block is suited for neuromorphic hardware as it can deal with mismatch both the sensory and the processing level.
%     \item we were able to show that device mismatch present on analogue neuromorphic hardware is useful feature if is combined with syanptic plasticity
%     \item the proposed meta-architecture opens novel perspective and understanding how temporal information present in event-based computing paradigm can be explicitely represented in ongoing processing
% \end{itemize}

\newpage
\section*{Postamble}
\lettrine[lines=2]{W}{e} were able to show, by incorporating event-driven plasticity mechanisms in recurrently connected networks of recurrently connected excitatory-inhibitory neuron populations, that this meta-network is capable of extracting statistically relevant spatio-temporal features and their temporal sequence from asynchronous event-based inputs.
The proposed meta-architecture was inspired by the highly recurrent, stereotypically repeating canonical organisation structures found in the neocortex (see Section~\ref{sec:microcircuit}).
Instead of making any prior connectivity assumptions within the network by hard-coding synaptic projections and hand-tuning their parameters, as we did in previous attempts to visual scene understanding (see Chapter~\ref{ch:reactivecontrol} \&~\ref{ch:semd}), we installed unsupervised event-based plasticity rules such that the system can self-adjust its free parameters given the provided input (see Section~\ref{sec:plasticity}).\\
The reason why conducted this last study is two-fold: Firstly, we wanted to formulate a meta-architecture which is genuinely capable of learning spatio-temporal patterns irrespective of the sensory modality, but which only cares about precisely timed asynchronous streams of events.
Secondly, we intended to construct a modular building block which can be (re-)used at different levels of feature complexity as its primary objective is to learn and predict spatio-temporal patterns.
The installed prediction mechanism (see Section~\ref{sec:ase_octa}) can be understood as a locally encoded, self-generated error signal which in time-continuous systems is solely based on the precise time of predicted and incoming activity.
This self-generated error signal can be seen as internally generated appropriate behaviour for predicting future inputs.
The unsupervised nature of adjusting the network's parameters in combination with the recurrent organisation enabled the system to dynamically allocate its computational resources and installed an implicit temporal representation of time in ongoing processing.\\
We were able to further demonstrate that by incorporating inhomogeneous network parameters, i.e.\ device mismatch, in our simulation the system not only successfully prevents fatal oscillatory states due to correlation-based learning rules (see Figure~\ref{fig:mismatch} \&~\ref{fig:adp}), but more importantly that the learned parameters, i.e.\ synaptic weights, can be successfully mapped onto mixed-signal sub-threshold neuromorphic processors (see Figure~\ref{fig:quantised_weights} \&~\ref{fig:vmem_traces}).\\
Thereby, we achieved a formulation of a feature extraction system suited for event-based neuromorphic sensory-processing, to potentially learn spatio-temporal features relevant for object recognition and forming high-level semantic concepts.
Furthermore, we could show for the first time that device mismatch is a crucial feature for stable learning and computation and when combined with event-based plasticity rules can be exploited to allocate computational resources whose parameters best match the provided inputs.



% \begin{itemize}
    % \item we had to design it unsupervised so that we use the meta archtitecues to learn st pattern in general
    % \item to construct a modular building block which can be used at different levels of feature complexity as its primary objective is to learn and predict spatio-temporal patterns
    % \item prediction = self-generated teacher signal, which in time-continous systems is purely based on the time when a prediction was made.
    % \item dynamically allocate computational resources
    % \item extract statistically relevant information
    % \item learned phase shift as generated appropriate behaviour
    % \item implementable on neuromorphic hw
    % \item computational role of synaptic projections (sequence learning)
    % \item device mismatch is a feature not a bug
    % \item implicit temporal representation of time in ongoing processing.
% \end{itemize}
\)
\end{document}

-----END DOCUMENT-----

Log file:
-----BEGIN LOG-----
This is pdfTeX, Version 3.14159265-2.6-1.40.16 (TeX Live 2015/Debian) (preloaded format=pdflatex 2018.10.14)  7 MAR 2019 14:18
entering extended mode
 restricted \write18 enabled.
 %&-line parsing enabled.
**e28fd8f3ab382de957a3d82b4872829c.tex
(./e28fd8f3ab382de957a3d82b4872829c.tex
LaTeX2e <2016/02/01>
Babel <3.9q> and hyphenation patterns for 5 language(s) loaded.
(/usr/share/texlive/texmf-dist/tex/latex/standalone/standalone.cls
Document Class: standalone 2015/07/15 v1.2 Class to compile TeX sub-files stand
alone
(/usr/share/texlive/texmf-dist/tex/generic/oberdiek/ifluatex.sty
Package: ifluatex 2010/03/01 v1.3 Provides the ifluatex switch (HO)
Package ifluatex Info: LuaTeX not detected.
)
(/usr/share/texlive/texmf-dist/tex/generic/oberdiek/ifpdf.sty
Package: ifpdf 2011/01/30 v2.3 Provides the ifpdf switch (HO)
Package ifpdf Info: pdfTeX in PDF mode is detected.
)
(/usr/share/texlive/texmf-dist/tex/generic/ifxetex/ifxetex.sty
Package: ifxetex 2010/09/12 v0.6 Provides ifxetex conditional
)
(/usr/share/texlive/texmf-dist/tex/latex/xkeyval/xkeyval.sty
Package: xkeyval 2014/12/03 v2.7a package option processing (HA)

(/usr/share/texlive/texmf-dist/tex/generic/xkeyval/xkeyval.tex
(/usr/share/texlive/texmf-dist/tex/generic/xkeyval/xkvutils.tex
\XKV@toks=\toks14
\XKV@tempa@toks=\toks15

(/usr/share/texlive/texmf-dist/tex/generic/xkeyval/keyval.tex))
\XKV@depth=\count79
File: xkeyval.tex 2014/12/03 v2.7a key=value parser (HA)
))
\sa@internal=\count80
\c@sapage=\count81

(/usr/share/texlive/texmf-dist/tex/latex/standalone/standalone.cfg
File: standalone.cfg 2015/07/15 v1.2 Default configuration file for 'standalone
' class
)
(/usr/share/texlive/texmf-dist/tex/latex/base/article.cls
Document Class: article 2014/09/29 v1.4h Standard LaTeX document class
(/usr/share/texlive/texmf-dist/tex/latex/base/size10.clo
File: size10.clo 2014/09/29 v1.4h Standard LaTeX file (size option)
)
\c@part=\count82
\c@section=\count83
\c@subsection=\count84
\c@subsubsection=\count85
\c@paragraph=\count86
\c@subparagraph=\count87
\c@figure=\count88
\c@table=\count89
\abovecaptionskip=\skip41
\belowcaptionskip=\skip42
\bibindent=\dimen102
)
(/usr/share/texmf/tex/latex/preview/preview.sty
Package: preview 2010/02/14 11.88 (AUCTeX/preview-latex)

(/usr/share/texmf/tex/latex/preview/prtightpage.def
\PreviewBorder=\dimen103
)
\pr@snippet=\count90
\pr@box=\box26
\pr@output=\toks16
))
(/usr/share/texlive/texmf-dist/tex/latex/xcolor/xcolor.sty
Package: xcolor 2007/01/21 v2.11 LaTeX color extensions (UK)

(/usr/share/texlive/texmf-dist/tex/latex/latexconfig/color.cfg
File: color.cfg 2007/01/18 v1.5 color configuration of teTeX/TeXLive
)
Package xcolor Info: Driver file: pdftex.def on input line 225.

(/usr/share/texlive/texmf-dist/tex/latex/pdftex-def/pdftex.def
File: pdftex.def 2011/05/27 v0.06d Graphics/color for pdfTeX

(/usr/share/texlive/texmf-dist/tex/generic/oberdiek/infwarerr.sty
Package: infwarerr 2010/04/08 v1.3 Providing info/warning/error messages (HO)
)
(/usr/share/texlive/texmf-dist/tex/generic/oberdiek/ltxcmds.sty
Package: ltxcmds 2011/11/09 v1.22 LaTeX kernel commands for general use (HO)
)
\Gread@gobject=\count91
)
Package xcolor Info: Model `cmy' substituted by `cmy0' on input line 1337.
Package xcolor Info: Model `hsb' substituted by `rgb' on input line 1341.
Package xcolor Info: Model `RGB' extended on input line 1353.
Package xcolor Info: Model `HTML' substituted by `rgb' on input line 1355.
Package xcolor Info: Model `Hsb' substituted by `hsb' on input line 1356.
Package xcolor Info: Model `tHsb' substituted by `hsb' on input line 1357.
Package xcolor Info: Model `HSB' substituted by `hsb' on input line 1358.
Package xcolor Info: Model `Gray' substituted by `gray' on input line 1359.
Package xcolor Info: Model `wave' substituted by `hsb' on input line 1360.
)
(/usr/share/texlive/texmf-dist/tex/latex/amsmath/amsmath.sty
Package: amsmath 2016/03/03 v2.15a AMS math features
\@mathmargin=\skip43

For additional information on amsmath, use the `?' option.
(/usr/share/texlive/texmf-dist/tex/latex/amsmath/amstext.sty
Package: amstext 2000/06/29 v2.01 AMS text

(/usr/share/texlive/texmf-dist/tex/latex/amsmath/amsgen.sty
File: amsgen.sty 1999/11/30 v2.0 generic functions
\@emptytoks=\toks17
\ex@=\dimen104
))
(/usr/share/texlive/texmf-dist/tex/latex/amsmath/amsbsy.sty
Package: amsbsy 1999/11/29 v1.2d Bold Symbols
\pmbraise@=\dimen105
)
(/usr/share/texlive/texmf-dist/tex/latex/amsmath/amsopn.sty
Package: amsopn 1999/12/14 v2.01 operator names
)
\inf@bad=\count92
LaTeX Info: Redefining \frac on input line 199.
\uproot@=\count93
\leftroot@=\count94
LaTeX Info: Redefining \overline on input line 297.
\classnum@=\count95
\DOTSCASE@=\count96
LaTeX Info: Redefining \ldots on input line 394.
LaTeX Info: Redefining \dots on input line 397.
LaTeX Info: Redefining \cdots on input line 518.
\Mathstrutbox@=\box27
\strutbox@=\box28
\big@size=\dimen106
LaTeX Font Info:    Redeclaring font encoding OML on input line 630.
LaTeX Font Info:    Redeclaring font encoding OMS on input line 631.
\macc@depth=\count97
\c@MaxMatrixCols=\count98
\dotsspace@=\muskip10
\c@parentequation=\count99
\dspbrk@lvl=\count100
\tag@help=\toks18
\row@=\count101
\column@=\count102
\maxfields@=\count103
\andhelp@=\toks19
\eqnshift@=\dimen107
\alignsep@=\dimen108
\tagshift@=\dimen109
\tagwidth@=\dimen110
\totwidth@=\dimen111
\lineht@=\dimen112
\@envbody=\toks20
\multlinegap=\skip44
\multlinetaggap=\skip45
\mathdisplay@stack=\toks21
LaTeX Info: Redefining \[ on input line 2735.
LaTeX Info: Redefining \] on input line 2736.
)
(/usr/share/texlive/texmf-dist/tex/latex/amsfonts/amssymb.sty
Package: amssymb 2013/01/14 v3.01 AMS font symbols

(/usr/share/texlive/texmf-dist/tex/latex/amsfonts/amsfonts.sty
Package: amsfonts 2013/01/14 v3.01 Basic AMSFonts support
\symAMSa=\mathgroup4
\symAMSb=\mathgroup5
LaTeX Font Info:    Overwriting math alphabet `\mathfrak' in version `bold'
(Font)                  U/euf/m/n --> U/euf/b/n on input line 106.
))
(/usr/share/texlive/texmf-dist/tex/latex/base/latexsym.sty
Package: latexsym 1998/08/17 v2.2e Standard LaTeX package (lasy symbols)
\symlasy=\mathgroup6
LaTeX Font Info:    Overwriting symbol font `lasy' in version `bold'
(Font)                  U/lasy/m/n --> U/lasy/b/n on input line 52.
)
(/usr/share/texlive/texmf-dist/tex/latex/mathtools/mathtools.sty
Package: mathtools 2015/11/12 v1.18 mathematical typesetting tools

(/usr/share/texlive/texmf-dist/tex/latex/tools/calc.sty
Package: calc 2014/10/28 v4.3 Infix arithmetic (KKT,FJ)
\calc@Acount=\count104
\calc@Bcount=\count105
\calc@Adimen=\dimen113
\calc@Bdimen=\dimen114
\calc@Askip=\skip46
\calc@Bskip=\skip47
LaTeX Info: Redefining \setlength on input line 80.
LaTeX Info: Redefining \addtolength on input line 81.
\calc@Ccount=\count106
\calc@Cskip=\skip48
)
(/usr/share/texlive/texmf-dist/tex/latex/mathtools/mhsetup.sty
Package: mhsetup 2010/01/21 v1.2a programming setup (MH)
)
LaTeX Info: Thecontrolsequence`\('isalreadyrobust on input line 129.
LaTeX Info: Thecontrolsequence`\)'isalreadyrobust on input line 129.
LaTeX Info: Thecontrolsequence`\['isalreadyrobust on input line 129.
LaTeX Info: Thecontrolsequence`\]'isalreadyrobust on input line 129.
\g_MT_multlinerow_int=\count107
\l_MT_multwidth_dim=\dimen115
\origjot=\skip49
\l_MT_shortvdotswithinadjustabove_dim=\dimen116
\l_MT_shortvdotswithinadjustbelow_dim=\dimen117
\l_MT_above_intertext_sep=\dimen118
\l_MT_below_intertext_sep=\dimen119
\l_MT_above_shortintertext_sep=\dimen120
\l_MT_below_shortintertext_sep=\dimen121
)
No file e28fd8f3ab382de957a3d82b4872829c.aux.
\openout1 = `e28fd8f3ab382de957a3d82b4872829c.aux'.

LaTeX Font Info:    Checking defaults for OML/cmm/m/it on input line 10.
LaTeX Font Info:    ... okay on input line 10.
LaTeX Font Info:    Checking defaults for T1/cmr/m/n on input line 10.
LaTeX Font Info:    ... okay on input line 10.
LaTeX Font Info:    Checking defaults for OT1/cmr/m/n on input line 10.
LaTeX Font Info:    ... okay on input line 10.
LaTeX Font Info:    Checking defaults for OMS/cmsy/m/n on input line 10.
LaTeX Font Info:    ... okay on input line 10.
LaTeX Font Info:    Checking defaults for OMX/cmex/m/n on input line 10.
LaTeX Font Info:    ... okay on input line 10.
LaTeX Font Info:    Checking defaults for U/cmr/m/n on input line 10.
LaTeX Font Info:    ... okay on input line 10.
Preview: Fontsize 10pt
Preview: PDFoutput 1
(/usr/share/texlive/texmf-dist/tex/context/base/supp-pdf.mkii
[Loading MPS to PDF converter (version 2006.09.02).]
\scratchcounter=\count108
\scratchdimen=\dimen122
\scratchbox=\box29
\nofMPsegments=\count109
\nofMParguments=\count110
\everyMPshowfont=\toks22
\MPscratchCnt=\count111
\MPscratchDim=\dimen123
\MPnumerator=\count112
\makeMPintoPDFobject=\count113
\everyMPtoPDFconversion=\toks23
) (/usr/share/texlive/texmf-dist/tex/latex/graphics/graphicx.sty
Package: graphicx 2014/10/28 v1.0g Enhanced LaTeX Graphics (DPC,SPQR)

(/usr/share/texlive/texmf-dist/tex/latex/graphics/graphics.sty
Package: graphics 2016/01/03 v1.0q Standard LaTeX Graphics (DPC,SPQR)

(/usr/share/texlive/texmf-dist/tex/latex/graphics/trig.sty
Package: trig 2016/01/03 v1.10 sin cos tan (DPC)
)
(/usr/share/texlive/texmf-dist/tex/latex/latexconfig/graphics.cfg
File: graphics.cfg 2010/04/23 v1.9 graphics configuration of TeX Live
)
Package graphics Info: Driver file: pdftex.def on input line 95.
)
(/usr/share/texlive/texmf-dist/tex/generic/oberdiek/pdftexcmds.sty
Package: pdftexcmds 2011/11/29 v0.20 Utility functions of pdfTeX for LuaTeX (HO
)
Package pdftexcmds Info: LuaTeX not detected.
Package pdftexcmds Info: \pdf@primitive is available.
Package pdftexcmds Info: \pdf@ifprimitive is available.
Package pdftexcmds Info: \pdfdraftmode found.
)
(/usr/share/texlive/texmf-dist/tex/latex/oberdiek/epstopdf-base.sty
Package: epstopdf-base 2010/02/09 v2.5 Base part for package epstopdf

(/usr/share/texlive/texmf-dist/tex/latex/oberdiek/grfext.sty
Package: grfext 2010/08/19 v1.1 Manage graphics extensions (HO)

(/usr/share/texlive/texmf-dist/tex/generic/oberdiek/kvdefinekeys.sty
Package: kvdefinekeys 2011/04/07 v1.3 Define keys (HO)
))
(/usr/share/texlive/texmf-dist/tex/latex/oberdiek/kvoptions.sty
Package: kvoptions 2011/06/30 v3.11 Key value format for package options (HO)

(/usr/share/texlive/texmf-dist/tex/generic/oberdiek/kvsetkeys.sty
Package: kvsetkeys 2012/04/25 v1.16 Key value parser (HO)

(/usr/share/texlive/texmf-dist/tex/generic/oberdiek/etexcmds.sty
Package: etexcmds 2011/02/16 v1.5 Avoid name clashes with e-TeX commands (HO)
Package etexcmds Info: Could not find \expanded.
(etexcmds)             That can mean that you are not using pdfTeX 1.50 or
(etexcmds)             that some package has redefined \expanded.
(etexcmds)             In the latter case, load this package earlier.
)))
Package grfext Info: Graphics extension search list:
(grfext)             [.png,.pdf,.jpg,.mps,.jpeg,.jbig2,.jb2,.PNG,.PDF,.JPG,.JPE
G,.JBIG2,.JB2,.eps]
(grfext)             \AppendGraphicsExtensions on input line 452.

(/usr/share/texlive/texmf-dist/tex/latex/latexconfig/epstopdf-sys.cfg
File: epstopdf-sys.cfg 2010/07/13 v1.3 Configuration of (r)epstopdf for TeX Liv
e
))
\Gin@req@height=\dimen124
\Gin@req@width=\dimen125
)
LaTeX Font Info:    Try loading font information for U+msa on input line 13.

(/usr/share/texlive/texmf-dist/tex/latex/amsfonts/umsa.fd
File: umsa.fd 2013/01/14 v3.01 AMS symbols A
)
LaTeX Font Info:    Try loading font information for U+msb on input line 13.

(/usr/share/texlive/texmf-dist/tex/latex/amsfonts/umsb.fd
File: umsb.fd 2013/01/14 v3.01 AMS symbols B
)
LaTeX Font Info:    Try loading font information for U+lasy on input line 13.

(/usr/share/texlive/texmf-dist/tex/latex/base/ulasy.fd
File: ulasy.fd 1998/08/17 v2.2e LaTeX symbol font definitions
))
Runaway argument?
{(1 - \phi ), \end {equation} where $\tau _{pred}$ is the time consta\ETC.
! File ended while scanning use of \frac .
<inserted text> 
                \par 
<*> e28fd8f3ab382de957a3d82b4872829c.tex
                                        
I suspect you have forgotten a `}', causing me
to read past where you wanted me to stop.
I'll try to recover; but if the error is serious,
you'd better type `E' or `X' now and fix your file.

! Emergency stop.
<*> e28fd8f3ab382de957a3d82b4872829c.tex
                                        
*** (job aborted, no legal \end found)

 
Here is how much of TeX's memory you used:
 3922 strings out of 494910
 55276 string characters out of 6179835
 155368 words of memory out of 5000000
 7153 multiletter control sequences out of 15000+600000
 5657 words of font info for 25 fonts, out of 8000000 for 9000
 36 hyphenation exceptions out of 8191
 51i,4n,56p,1415b,134s stack positions out of 5000i,500n,10000p,200000b,80000s
!  ==> Fatal error occurred, no output PDF file produced!

-----END LOG-----